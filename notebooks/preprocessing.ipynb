{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEXT\n",
    "* remove_null\n",
    "* lower case\n",
    "* stem and lemmatize\n",
    "\n",
    "## IMAGE\n",
    "* resize all and save (grayscale)\n",
    "* augmentation using GANs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/sameep/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "2023-03-17 15:47:20.445594: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-17 15:47:20.706581: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-03-17 15:47:20.710061: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-17 15:47:20.710077: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-17 15:47:21.472900: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-17 15:47:21.472987: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-17 15:47:21.472993: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "import os\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "import tensorflow as tf\n",
    "from PIL import Image, ImageFilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16395</td>\n",
       "      <td>handjob sold seper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37405</td>\n",
       "      <td>introduc fidget spinner for woman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>94180</td>\n",
       "      <td>happi pride month let' go beat up lesbian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54321</td>\n",
       "      <td>laugh in [major of u. crime rate]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>97015</td>\n",
       "      <td>find out those 72 virgins.. are goat</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                       text\n",
       "0  16395                         handjob sold seper\n",
       "1  37405          introduc fidget spinner for woman\n",
       "2  94180  happi pride month let' go beat up lesbian\n",
       "3  54321          laugh in [major of u. crime rate]\n",
       "4  97015       find out those 72 virgins.. are goat"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('../data/facebook/train.csv')\n",
    "train_df.drop('Unnamed: 0', axis = 1, inplace = True)\n",
    "\n",
    "test_df = pd.read_csv('../data/facebook/test.csv')\n",
    "test_df.drop('Unnamed: 0', axis = 1, inplace = True)\n",
    "\n",
    "val_df = pd.read_csv('../data/facebook/val.csv')\n",
    "val_df.drop('Unnamed: 0', axis = 1, inplace = True)\n",
    "\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Text_Preprocessor:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def get_df(self):\n",
    "        return self.df\n",
    "    \n",
    "    def lower_case(self):\n",
    "        self.df['text'] = self.df['text'].apply(str.lower)\n",
    "\n",
    "    def remove_punctuations(self):\n",
    "        cleaned_text = []\n",
    "        for index in tqdm(range(self.df.shape[0])):\n",
    "            text = self.df['text'].iloc[index]\n",
    "\n",
    "            word_tokens = text.split()\n",
    "            \n",
    "            table = str.maketrans('', '', string.punctuation)\n",
    "            stripped = [w.translate(table) for w in word_tokens]\n",
    "\n",
    "            filtered_sentence = \" \".join(stripped).strip()\n",
    "            cleaned_text.append(filtered_sentence)\n",
    "        self.df['text'] = np.array(cleaned_text)\n",
    "\n",
    "    def remove_null(self):\n",
    "        for col in self.df.columns:\n",
    "            if(self.df[col].isnull().sum() > 0):\n",
    "                self.df.dropna(inplace = True)\n",
    "\n",
    "    def stem_and_lemmatize(self):\n",
    "        cleaned_text = []\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        ps = PorterStemmer()\n",
    "\n",
    "        for index in tqdm(range(self.df.shape[0])):\n",
    "            text = self.df['text'].iloc[index]\n",
    "\n",
    "            word_tokens = text.split()\n",
    "\n",
    "            for count, word in enumerate(word_tokens):\n",
    "                temp = lemmatizer.lemmatize(word)\n",
    "                word_tokens[count] = ps.stem(temp)\n",
    "\n",
    "            filtered_sentence = \" \".join(word_tokens).strip()\n",
    "            cleaned_text.append(filtered_sentence)\n",
    "        \n",
    "        self.df['text'] = np.array(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8500/8500 [00:00<00:00, 63393.44it/s]\n",
      "100%|██████████| 8500/8500 [00:02<00:00, 2918.68it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 70449.88it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 5904.61it/s]\n",
      "100%|██████████| 500/500 [00:00<00:00, 68395.80it/s]\n",
      "100%|██████████| 500/500 [00:00<00:00, 4146.50it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>42953</td>\n",
       "      <td>it their charact not their color that matter</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23058</td>\n",
       "      <td>dont be afraid to love again everyon is not li...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13894</td>\n",
       "      <td>put bow on your pet</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37408</td>\n",
       "      <td>i love everyth and everybodi except for squirr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>82403</td>\n",
       "      <td>everybodi love chocol chip cooki even hitler</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                               text  label\n",
       "0  42953       it their charact not their color that matter      0\n",
       "1  23058  dont be afraid to love again everyon is not li...      0\n",
       "2  13894                                put bow on your pet      0\n",
       "3  37408  i love everyth and everybodi except for squirr...      0\n",
       "4  82403       everybodi love chocol chip cooki even hitler      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text_preprocessor = Text_Preprocessor(train_df)\n",
    "train_text_preprocessor.lower_case()\n",
    "train_text_preprocessor.remove_null()\n",
    "train_text_preprocessor.remove_punctuations()\n",
    "train_text_preprocessor.stem_and_lemmatize()\n",
    "train_df = train_text_preprocessor.get_df()\n",
    "\n",
    "test_text_preprocessor = Text_Preprocessor(test_df)\n",
    "test_text_preprocessor.lower_case()\n",
    "test_text_preprocessor.remove_null()\n",
    "test_text_preprocessor.remove_punctuations()\n",
    "test_text_preprocessor.stem_and_lemmatize()\n",
    "test_df = test_text_preprocessor.get_df()\n",
    "\n",
    "val_text_preprocessor = Text_Preprocessor(val_df)\n",
    "val_text_preprocessor.lower_case()\n",
    "val_text_preprocessor.remove_null()\n",
    "val_text_preprocessor.remove_punctuations()\n",
    "val_text_preprocessor.stem_and_lemmatize()\n",
    "val_df = val_text_preprocessor.get_df()\n",
    "\n",
    "train_df.to_csv('../data/facebook/train.csv')\n",
    "test_df.to_csv('../data/facebook/test.csv')\n",
    "val_df.to_csv('../data/facebook/val.csv')\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Image_Preprocessing:\n",
    "    def __init__(self, df, dir_name):\n",
    "        self.df = df\n",
    "        self.dir_name = dir_name\n",
    "    \n",
    "    def get_df(self):\n",
    "        return self.df\n",
    "    \n",
    "    def resize_and_grayscale(self):\n",
    "        for id in tqdm(self.df['id']):\n",
    "            file_name = str(id) + '.png'\n",
    "            _image = Image.open(os.path.join('../data/facebook/img', self.dir_name, file_name))\n",
    "            _image = _image.convert('L')\n",
    "            _image = _image.resize((224, 224))\n",
    "            _image.save(os.path.join('../data/facebook/img', self.dir_name, file_name))\n",
    "            \n",
    "\n",
    "    def __rotate(self, id):\n",
    "        file_name = str(id) + '.png'\n",
    "        new_file_name = str(id) + \"_augmented.png\"\n",
    "        path = os.path.join('../data/facebook/img', self.dir_name, file_name)\n",
    "        _image = np.array(Image.open(path))\n",
    "        angle = tf.random.uniform([], minval=-60, maxval=60)\n",
    "        _image = tf.keras.preprocessing.image.random_rotation(_image, angle, row_axis=0, col_axis=1, channel_axis=2)\n",
    "        _image = Image.fromarray(_image)\n",
    "        _image.save(os.path.join('../data/facebook/img', self.dir_name, new_file_name))\n",
    "        temp_df = self.df.loc[self.df['id'] == id]\n",
    "        text = temp_df.loc[temp_df.iloc[0].name, 'text']\n",
    "        new_id = new_file_name.strip('.png')\n",
    "        if 'label' in self.df.columns:\n",
    "            label = temp_df.loc[temp_df.iloc[0].name, 'label']\n",
    "            self.df.loc[len(self.df.index)] = [new_id, text, label]\n",
    "        else:\n",
    "            self.df.loc[len(self.df.index)] = [new_id, text]\n",
    "\n",
    "    def __gaussian_noise(self, id):\n",
    "        file_name = str(id) + '.png'\n",
    "        new_file_name = str(id) + \"_augmented.png\"\n",
    "        path = os.path.join('../data/facebook/img', self.dir_name, file_name)\n",
    "        _image = np.array(Image.open(path))\n",
    "        noise = tf.random.normal(shape=tf.shape(_image), mean=0, stddev=50, dtype=tf.float32)\n",
    "        _image = tf.clip_by_value(_image + noise, 0, 1)\n",
    "        _image = _image*255\n",
    "        _image = np.array(_image, dtype=np.uint8)\n",
    "        _image = Image.fromarray(_image)\n",
    "        _image.save(os.path.join('../data/facebook/img/', self.dir_name, new_file_name))\n",
    "        temp_df = self.df.loc[self.df['id'] == id]\n",
    "        text = temp_df.loc[temp_df.iloc[0].name, 'text']\n",
    "        new_id = new_file_name.strip('.png')\n",
    "        if 'label' in self.df.columns:\n",
    "            label = temp_df.loc[temp_df.iloc[0].name, 'label']\n",
    "            self.df.loc[len(self.df.index)] = [new_id, text, label]\n",
    "        else:\n",
    "            self.df.loc[len(self.df.index)] = [new_id, text]\n",
    "\n",
    "    def __blur(self, id):\n",
    "        file_name = str(id) + '.png'\n",
    "        new_file_name = str(id) + \"_augmented.png\"\n",
    "        path = os.path.join('../data/facebook/img', self.dir_name, file_name)\n",
    "        _image = (Image.open(path))\n",
    "        _image = _image.filter(ImageFilter.BLUR)\n",
    "        _image.save(os.path.join('../data/facebook/img/', self.dir_name, new_file_name))\n",
    "        temp_df = self.df.loc[self.df['id'] == id]\n",
    "        text = temp_df.loc[temp_df.iloc[0].name, 'text']\n",
    "        new_id = new_file_name.strip('.png')\n",
    "        if 'label' in self.df.columns:\n",
    "            label = temp_df.loc[temp_df.iloc[0].name, 'label']\n",
    "            self.df.loc[len(self.df.index)] = [new_id, text, label]\n",
    "        else:\n",
    "            self.df.loc[len(self.df.index)] = [new_id, text]\n",
    "\n",
    "    def augmentation(self):\n",
    "        for id in tqdm(self.df['id']):\n",
    "            # Select a random augmentation\n",
    "            augmentations = [self.__rotate, self.__gaussian_noise, self.__blur]\n",
    "            selected_augmentation = np.random.choice(augmentations)\n",
    "\n",
    "            # Apply the selected augmentation\n",
    "            selected_augmentation(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8500 [00:00<?, ?it/s]2023-03-17 15:47:25.809188: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-03-17 15:47:25.809213: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-03-17 15:47:25.809234: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (sameep-Inspiron-5593): /proc/driver/nvidia/version does not exist\n",
      "2023-03-17 15:47:25.809476: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "100%|██████████| 8500/8500 [22:39<00:00,  6.25it/s]  \n",
      "100%|██████████| 17000/17000 [08:56<00:00, 31.69it/s]\n",
      "100%|██████████| 1000/1000 [04:12<00:00,  3.96it/s]\n",
      "100%|██████████| 2000/2000 [01:24<00:00, 23.65it/s]\n",
      "100%|██████████| 500/500 [01:54<00:00,  4.35it/s]\n",
      "100%|██████████| 1000/1000 [00:41<00:00, 24.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 41min 41s, sys: 25.8 s, total: 42min 7s\n",
      "Wall time: 39min 49s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_img_preprocessor = Image_Preprocessing(train_df, \"train\")\n",
    "train_img_preprocessor.augmentation()\n",
    "train_img_preprocessor.resize_and_grayscale()\n",
    "train_df = train_img_preprocessor.get_df()\n",
    "train_df.to_csv('../data/facebook/train.csv')\n",
    "\n",
    "\n",
    "test_img_preprocessor = Image_Preprocessing(test_df, \"test\")\n",
    "test_img_preprocessor.augmentation()\n",
    "test_img_preprocessor.resize_and_grayscale()\n",
    "test_df = test_img_preprocessor.get_df()\n",
    "test_df.to_csv('../data/facebook/test.csv')\n",
    "\n",
    "\n",
    "val_img_preprocessor = Image_Preprocessing(val_df, \"val\")\n",
    "val_img_preprocessor.augmentation()\n",
    "val_img_preprocessor.resize_and_grayscale()\n",
    "val_df = val_img_preprocessor.get_df()\n",
    "val_df.to_csv('../data/facebook/val.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HMC",
   "language": "python",
   "name": "hmc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
