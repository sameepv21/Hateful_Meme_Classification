{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEXT\n",
    "* remove_null\n",
    "* lower case\n",
    "* stem and lemmatize\n",
    "\n",
    "## IMAGE\n",
    "* resize all and save (grayscale)\n",
    "* augmentation using GANs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/sameep/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "2023-03-31 22:17:57.811510: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-31 22:17:58.128145: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-03-31 22:17:58.155328: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-31 22:17:58.155352: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-31 22:17:59.096912: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-31 22:17:59.097030: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-31 22:17:59.097038: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "import os\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "import tensorflow as tf\n",
    "from PIL import Image, ImageFilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>img</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16395</td>\n",
       "      <td>img/16395.png</td>\n",
       "      <td>handjobs sold seperately</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37405</td>\n",
       "      <td>img/37405.png</td>\n",
       "      <td>introducing fidget spinner for women</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>94180</td>\n",
       "      <td>img/94180.png</td>\n",
       "      <td>happy pride month let's go beat up lesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54321</td>\n",
       "      <td>img/54321.png</td>\n",
       "      <td>laughs in [majority of u.s crime rate]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>97015</td>\n",
       "      <td>img/97015.png</td>\n",
       "      <td>finds out those 72 virgins.. are goats</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id            img                                         text\n",
       "0  16395  img/16395.png                     handjobs sold seperately\n",
       "1  37405  img/37405.png         introducing fidget spinner for women\n",
       "2  94180  img/94180.png  happy pride month let's go beat up lesbians\n",
       "3  54321  img/54321.png       laughs in [majority of u.s crime rate]\n",
       "4  97015  img/97015.png       finds out those 72 virgins.. are goats"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_json('../data/facebook/train.jsonl', lines = True)\n",
    "\n",
    "test_df = pd.read_json('../data/facebook/test.jsonl', lines = True)\n",
    "\n",
    "dev_df = pd.read_json('../data/facebook/dev.jsonl', lines = True)\n",
    "\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Text_Preprocessor:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def get_df(self):\n",
    "        return self.df\n",
    "    \n",
    "    def lower_case(self):\n",
    "        self.df['text'] = self.df['text'].apply(str.lower)\n",
    "\n",
    "    def remove_punctuations(self):\n",
    "        cleaned_text = []\n",
    "        for index in tqdm(range(self.df.shape[0])):\n",
    "            text = self.df['text'].iloc[index]\n",
    "\n",
    "            word_tokens = text.split()\n",
    "            \n",
    "            table = str.maketrans('', '', string.punctuation)\n",
    "            stripped = [w.translate(table) for w in word_tokens]\n",
    "\n",
    "            filtered_sentence = \" \".join(stripped).strip()\n",
    "            cleaned_text.append(filtered_sentence)\n",
    "        self.df['text'] = np.array(cleaned_text)\n",
    "\n",
    "    def remove_null(self):\n",
    "        for col in self.df.columns:\n",
    "            if(self.df[col].isnull().sum() > 0):\n",
    "                self.df.dropna(inplace = True)\n",
    "\n",
    "    def stem_and_lemmatize(self):\n",
    "        cleaned_text = []\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        ps = PorterStemmer()\n",
    "\n",
    "        for index in tqdm(range(self.df.shape[0])):\n",
    "            text = self.df['text'].iloc[index]\n",
    "\n",
    "            word_tokens = text.split()\n",
    "\n",
    "            for count, word in enumerate(word_tokens):\n",
    "                temp = lemmatizer.lemmatize(word)\n",
    "                word_tokens[count] = ps.stem(temp)\n",
    "\n",
    "            filtered_sentence = \" \".join(word_tokens).strip()\n",
    "            cleaned_text.append(filtered_sentence)\n",
    "        \n",
    "        self.df['text'] = np.array(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8500/8500 [00:00<00:00, 40826.65it/s]\n",
      "100%|██████████| 8500/8500 [00:03<00:00, 2233.64it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 53790.37it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4718.27it/s]\n",
      "100%|██████████| 500/500 [00:00<00:00, 68520.94it/s]\n",
      "100%|██████████| 500/500 [00:00<00:00, 5432.70it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>img</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>42953</td>\n",
       "      <td>img/42953.png</td>\n",
       "      <td>0</td>\n",
       "      <td>it their charact not their color that matter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23058</td>\n",
       "      <td>img/23058.png</td>\n",
       "      <td>0</td>\n",
       "      <td>dont be afraid to love again everyon is not li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13894</td>\n",
       "      <td>img/13894.png</td>\n",
       "      <td>0</td>\n",
       "      <td>put bow on your pet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37408</td>\n",
       "      <td>img/37408.png</td>\n",
       "      <td>0</td>\n",
       "      <td>i love everyth and everybodi except for squirr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>82403</td>\n",
       "      <td>img/82403.png</td>\n",
       "      <td>0</td>\n",
       "      <td>everybodi love chocol chip cooki even hitler</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id            img  label  \\\n",
       "0  42953  img/42953.png      0   \n",
       "1  23058  img/23058.png      0   \n",
       "2  13894  img/13894.png      0   \n",
       "3  37408  img/37408.png      0   \n",
       "4  82403  img/82403.png      0   \n",
       "\n",
       "                                                text  \n",
       "0       it their charact not their color that matter  \n",
       "1  dont be afraid to love again everyon is not li...  \n",
       "2                                put bow on your pet  \n",
       "3  i love everyth and everybodi except for squirr...  \n",
       "4       everybodi love chocol chip cooki even hitler  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text_preprocessor = Text_Preprocessor(train_df)\n",
    "train_text_preprocessor.lower_case()\n",
    "train_text_preprocessor.remove_null()\n",
    "train_text_preprocessor.remove_punctuations()\n",
    "train_text_preprocessor.stem_and_lemmatize()\n",
    "train_df = train_text_preprocessor.get_df()\n",
    "\n",
    "test_text_preprocessor = Text_Preprocessor(test_df)\n",
    "test_text_preprocessor.lower_case()\n",
    "test_text_preprocessor.remove_null()\n",
    "test_text_preprocessor.remove_punctuations()\n",
    "test_text_preprocessor.stem_and_lemmatize()\n",
    "test_df = test_text_preprocessor.get_df()\n",
    "\n",
    "dev_text_preprocessor = Text_Preprocessor(dev_df)\n",
    "dev_text_preprocessor.lower_case()\n",
    "dev_text_preprocessor.remove_null()\n",
    "dev_text_preprocessor.remove_punctuations()\n",
    "dev_text_preprocessor.stem_and_lemmatize()\n",
    "dev_df = dev_text_preprocessor.get_df()\n",
    "\n",
    "train_df.to_json('../data/facebook/train.jsonl')\n",
    "test_df.to_json('../data/facebook/test.jsonl')\n",
    "dev_df.to_json('../data/facebook/dev.jsonl')\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Image_Preprocessing:\n",
    "    def __init__(self, df, dir_name):\n",
    "        self.df = df\n",
    "        self.dir_name = dir_name\n",
    "    \n",
    "    def get_df(self):\n",
    "        return self.df\n",
    "    \n",
    "    def resize_and_grayscale(self):\n",
    "        for img in tqdm(self.df['img']):\n",
    "            _image = Image.open(os.path.join('../data/facebook', img))\n",
    "            _image = _image.resize((224, 224))\n",
    "            _image.save(os.path.join('../data/facebook', img))\n",
    "            \n",
    "\n",
    "    def __rotate(self, img):\n",
    "        new_file_name = \"img/\" + img.strip('.pngim') + \"_augmented.png\"\n",
    "        path = os.path.join('../data/facebook', img)\n",
    "        \n",
    "        _image = np.array(Image.open(path))\n",
    "        angle = tf.random.uniform([], minval=-60, maxval=60)\n",
    "        _image = tf.keras.preprocessing.image.random_rotation(_image, angle, row_axis=0, col_axis=1, channel_axis=2)\n",
    "        _image = Image.fromarray(_image)\n",
    "        _image.save(os.path.join('../data/facebook', new_file_name))\n",
    "        \n",
    "        temp_df = self.df.loc[self.df['img'] == img]\n",
    "        text = temp_df.loc[temp_df.iloc[0].name, 'text']\n",
    "        id = temp_df.loc[temp_df.iloc[0].name, 'id']\n",
    "        if 'label' in self.df.columns:\n",
    "            label = temp_df.loc[temp_df.iloc[0].name, 'label']\n",
    "            self.df.loc[len(self.df.index)] = [id, new_file_name, label, text]\n",
    "        else:\n",
    "            self.df.loc[len(self.df.index)] = [id, new_file_name, text]\n",
    "\n",
    "    def __gaussian_noise(self, img):\n",
    "        new_file_name = \"img/\" + img.strip(\".pngim\") + \"_augmented.png\"\n",
    "        path = os.path.join('../data/facebook', img)\n",
    "        \n",
    "        _image = np.array(Image.open(path))\n",
    "        noise = tf.random.normal(shape=tf.shape(_image), mean=0, stddev=50, dtype=tf.float32)\n",
    "        _image = tf.clip_by_value(_image + noise, 0, 1)\n",
    "        _image = _image*255\n",
    "        _image = np.array(_image, dtype=np.uint8)\n",
    "        _image = Image.fromarray(_image)\n",
    "        _image.save(os.path.join('../data/facebook/', new_file_name))\n",
    "        \n",
    "        temp_df = self.df.loc[self.df['img'] == img]\n",
    "        text = temp_df.loc[temp_df.iloc[0].name, 'text']\n",
    "        id = temp_df.loc[temp_df.iloc[0].name, 'id']\n",
    "        if 'label' in self.df.columns:\n",
    "            label = temp_df.loc[temp_df.iloc[0].name, 'label']\n",
    "            self.df.loc[len(self.df.index)] = [id, new_file_name, label, text]\n",
    "        else:\n",
    "            self.df.loc[len(self.df.index)] = [id, new_file_name, text]\n",
    "\n",
    "    def __blur(self, img):\n",
    "        new_file_name = \"img/\" + img.strip(\".pngim\") + \"_augmented.png\"\n",
    "        path = os.path.join('../data/facebook', img)\n",
    "        \n",
    "        _image = (Image.open(path))\n",
    "        _image = _image.filter(ImageFilter.BLUR)\n",
    "        _image.save(os.path.join('../data/facebook/', new_file_name))\n",
    "        \n",
    "        temp_df = self.df.loc[self.df['img'] == img]\n",
    "        text = temp_df.loc[temp_df.iloc[0].name, 'text']\n",
    "        id = temp_df.loc[temp_df.iloc[0].name, 'id']\n",
    "        if 'label' in self.df.columns:\n",
    "            label = temp_df.loc[temp_df.iloc[0].name, 'label']\n",
    "            self.df.loc[len(self.df.index)] = [id, new_file_name, label, text]\n",
    "        else:\n",
    "            self.df.loc[len(self.df.index)] = [id, new_file_name, text]\n",
    "\n",
    "    def augmentation(self):\n",
    "        for img in tqdm(self.df['img']):\n",
    "            # Select a random augmentation\n",
    "            augmentations = [self.__rotate, self.__gaussian_noise, self.__blur]\n",
    "            selected_augmentation = np.random.choice(augmentations)\n",
    "\n",
    "            # Apply the selected augmentation\n",
    "            selected_augmentation(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8500 [00:00<?, ?it/s]2023-03-31 22:18:04.848702: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-03-31 22:18:04.848757: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-03-31 22:18:04.848817: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (sameep-Inspiron-5593): /proc/driver/nvidia/version does not exist\n",
      "2023-03-31 22:18:04.850598: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "100%|██████████| 8500/8500 [30:24<00:00,  4.66it/s]  \n",
      "100%|██████████| 17000/17000 [14:19<00:00, 19.78it/s]\n",
      "100%|██████████| 1000/1000 [05:43<00:00,  2.91it/s]\n",
      "100%|██████████| 2000/2000 [02:37<00:00, 12.73it/s]\n",
      "100%|██████████| 500/500 [02:58<00:00,  2.81it/s]\n",
      "100%|██████████| 1000/1000 [01:20<00:00, 12.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 59min 13s, sys: 41 s, total: 59min 54s\n",
      "Wall time: 57min 23s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_img_preprocessor = Image_Preprocessing(train_df, \"train\")\n",
    "train_img_preprocessor.augmentation()\n",
    "train_img_preprocessor.resize_and_grayscale()\n",
    "train_df = train_img_preprocessor.get_df()\n",
    "\n",
    "test_img_preprocessor = Image_Preprocessing(test_df, \"test\")\n",
    "test_img_preprocessor.augmentation()\n",
    "test_img_preprocessor.resize_and_grayscale()\n",
    "test_df = test_img_preprocessor.get_df()\n",
    "\n",
    "dev_img_preprocessor = Image_Preprocessing(dev_df, \"dev\")\n",
    "dev_img_preprocessor.augmentation()\n",
    "dev_img_preprocessor.resize_and_grayscale()\n",
    "dev_df = dev_img_preprocessor.get_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_json('../data/facebook/train.jsonl', orient = 'records')\n",
    "test_df.to_json('../data/facebook/test.jsonl', orient = 'records')\n",
    "dev_df.to_json('../data/facebook/dev.jsonl', orient = 'records')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HMC",
   "language": "python",
   "name": "hmc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
