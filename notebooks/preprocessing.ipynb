{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEXT\n",
    "* remove_null\n",
    "* lower case\n",
    "* stem and lemmatize\n",
    "\n",
    "## IMAGE\n",
    "* resize all and save (grayscale)\n",
    "* augmentation using GANs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "import os\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "import tensorflow as tf\n",
    "from PIL import Image, ImageFilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_json('../data/facebook/train.jsonl', lines = True)\n",
    "\n",
    "test_df = pd.read_json('../data/facebook/test.jsonl', lines = True)\n",
    "\n",
    "dev_df = pd.read_json('../data/facebook/dev.jsonl', lines = True)\n",
    "\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Text_Preprocessor:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def get_df(self):\n",
    "        return self.df\n",
    "    \n",
    "    def lower_case(self):\n",
    "        self.df['text'] = self.df['text'].apply(str.lower)\n",
    "\n",
    "    def remove_punctuations(self):\n",
    "        cleaned_text = []\n",
    "        for index in tqdm(range(self.df.shape[0])):\n",
    "            text = self.df['text'].iloc[index]\n",
    "\n",
    "            word_tokens = text.split()\n",
    "            \n",
    "            table = str.maketrans('', '', string.punctuation)\n",
    "            stripped = [w.translate(table) for w in word_tokens]\n",
    "\n",
    "            filtered_sentence = \" \".join(stripped).strip()\n",
    "            cleaned_text.append(filtered_sentence)\n",
    "        self.df['text'] = np.array(cleaned_text)\n",
    "\n",
    "    def remove_null(self):\n",
    "        for col in self.df.columns:\n",
    "            if(self.df[col].isnull().sum() > 0):\n",
    "                self.df.dropna(inplace = True)\n",
    "\n",
    "    def stem_and_lemmatize(self):\n",
    "        cleaned_text = []\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        ps = PorterStemmer()\n",
    "\n",
    "        for index in tqdm(range(self.df.shape[0])):\n",
    "            text = self.df['text'].iloc[index]\n",
    "\n",
    "            word_tokens = text.split()\n",
    "\n",
    "            for count, word in enumerate(word_tokens):\n",
    "                temp = lemmatizer.lemmatize(word)\n",
    "                word_tokens[count] = ps.stem(temp)\n",
    "\n",
    "            filtered_sentence = \" \".join(word_tokens).strip()\n",
    "            cleaned_text.append(filtered_sentence)\n",
    "        \n",
    "        self.df['text'] = np.array(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_preprocessor = Text_Preprocessor(train_df)\n",
    "train_text_preprocessor.lower_case()\n",
    "train_text_preprocessor.remove_null()\n",
    "train_text_preprocessor.remove_punctuations()\n",
    "train_text_preprocessor.stem_and_lemmatize()\n",
    "train_df = train_text_preprocessor.get_df()\n",
    "\n",
    "test_text_preprocessor = Text_Preprocessor(test_df)\n",
    "test_text_preprocessor.lower_case()\n",
    "test_text_preprocessor.remove_null()\n",
    "test_text_preprocessor.remove_punctuations()\n",
    "test_text_preprocessor.stem_and_lemmatize()\n",
    "test_df = test_text_preprocessor.get_df()\n",
    "\n",
    "dev_text_preprocessor = Text_Preprocessor(dev_df)\n",
    "dev_text_preprocessor.lower_case()\n",
    "dev_text_preprocessor.remove_null()\n",
    "dev_text_preprocessor.remove_punctuations()\n",
    "dev_text_preprocessor.stem_and_lemmatize()\n",
    "dev_df = dev_text_preprocessor.get_df()\n",
    "\n",
    "train_df.to_json('../data/facebook/train.jsonl')\n",
    "test_df.to_json('../data/facebook/test.jsonl')\n",
    "dev_df.to_json('../data/facebook/dev.jsonl')\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Image_Preprocessing:\n",
    "    def __init__(self, df, dir_name):\n",
    "        self.df = df\n",
    "        self.dir_name = dir_name\n",
    "    \n",
    "    def get_df(self):\n",
    "        return self.df\n",
    "    \n",
    "    def resize_and_grayscale(self):\n",
    "        for img in tqdm(self.df['img']):\n",
    "            _image = Image.open(os.path.join('../data/facebook', img))\n",
    "            _image = _image.resize((224, 224))\n",
    "            _image.save(os.path.join('../data/facebook', img))\n",
    "            \n",
    "\n",
    "    def __rotate(self, img):\n",
    "        new_file_name = \"img/\" + img.strip('.pngim') + \"_augmented.png\"\n",
    "        path = os.path.join('../data/facebook', img)\n",
    "        \n",
    "        _image = np.array(Image.open(path))\n",
    "        angle = tf.random.uniform([], minval=-60, maxval=60)\n",
    "        _image = tf.keras.preprocessing.image.random_rotation(_image, angle, row_axis=0, col_axis=1, channel_axis=2)\n",
    "        _image = Image.fromarray(_image)\n",
    "        _image.save(os.path.join('../data/facebook', new_file_name))\n",
    "        \n",
    "        temp_df = self.df.loc[self.df['img'] == img]\n",
    "        text = temp_df.loc[temp_df.iloc[0].name, 'text']\n",
    "        id = temp_df.loc[temp_df.iloc[0].name, 'id']\n",
    "        if 'label' in self.df.columns:\n",
    "            label = temp_df.loc[temp_df.iloc[0].name, 'label']\n",
    "            self.df.loc[len(self.df.index)] = [id, new_file_name, label, text]\n",
    "        else:\n",
    "            self.df.loc[len(self.df.index)] = [id, new_file_name, text]\n",
    "\n",
    "    def __gaussian_noise(self, img):\n",
    "        new_file_name = \"img/\" + img.strip(\".pngim\") + \"_augmented.png\"\n",
    "        path = os.path.join('../data/facebook', img)\n",
    "        \n",
    "        _image = np.array(Image.open(path))\n",
    "        noise = tf.random.normal(shape=tf.shape(_image), mean=0, stddev=50, dtype=tf.float32)\n",
    "        _image = tf.clip_by_value(_image + noise, 0, 1)\n",
    "        _image = _image*255\n",
    "        _image = np.array(_image, dtype=np.uint8)\n",
    "        _image = Image.fromarray(_image)\n",
    "        _image.save(os.path.join('../data/facebook/', new_file_name))\n",
    "        \n",
    "        temp_df = self.df.loc[self.df['img'] == img]\n",
    "        text = temp_df.loc[temp_df.iloc[0].name, 'text']\n",
    "        id = temp_df.loc[temp_df.iloc[0].name, 'id']\n",
    "        if 'label' in self.df.columns:\n",
    "            label = temp_df.loc[temp_df.iloc[0].name, 'label']\n",
    "            self.df.loc[len(self.df.index)] = [id, new_file_name, label, text]\n",
    "        else:\n",
    "            self.df.loc[len(self.df.index)] = [id, new_file_name, text]\n",
    "\n",
    "    def __blur(self, img):\n",
    "        new_file_name = \"img/\" + img.strip(\".pngim\") + \"_augmented.png\"\n",
    "        path = os.path.join('../data/facebook', img)\n",
    "        \n",
    "        _image = (Image.open(path))\n",
    "        _image = _image.filter(ImageFilter.BLUR)\n",
    "        _image.save(os.path.join('../data/facebook/', new_file_name))\n",
    "        \n",
    "        temp_df = self.df.loc[self.df['img'] == img]\n",
    "        text = temp_df.loc[temp_df.iloc[0].name, 'text']\n",
    "        id = temp_df.loc[temp_df.iloc[0].name, 'id']\n",
    "        if 'label' in self.df.columns:\n",
    "            label = temp_df.loc[temp_df.iloc[0].name, 'label']\n",
    "            self.df.loc[len(self.df.index)] = [id, new_file_name, label, text]\n",
    "        else:\n",
    "            self.df.loc[len(self.df.index)] = [id, new_file_name, text]\n",
    "\n",
    "    def augmentation(self):\n",
    "        for img in tqdm(self.df['img']):\n",
    "            # Select a random augmentation\n",
    "            augmentations = [self.__rotate, self.__gaussian_noise, self.__blur]\n",
    "            selected_augmentation = np.random.choice(augmentations)\n",
    "\n",
    "            # Apply the selected augmentation\n",
    "            selected_augmentation(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "train_img_preprocessor = Image_Preprocessing(train_df, \"train\")\n",
    "train_img_preprocessor.augmentation()\n",
    "train_img_preprocessor.resize_and_grayscale()\n",
    "train_df = train_img_preprocessor.get_df()\n",
    "\n",
    "test_img_preprocessor = Image_Preprocessing(test_df, \"test\")\n",
    "test_img_preprocessor.augmentation()\n",
    "test_img_preprocessor.resize_and_grayscale()\n",
    "test_df = test_img_preprocessor.get_df()\n",
    "\n",
    "dev_img_preprocessor = Image_Preprocessing(dev_df, \"dev\")\n",
    "dev_img_preprocessor.augmentation()\n",
    "dev_img_preprocessor.resize_and_grayscale()\n",
    "dev_df = dev_img_preprocessor.get_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_json('../data/facebook/train.jsonl', orient = 'records')\n",
    "test_df.to_json('../data/facebook/test.jsonl', orient = 'records')\n",
    "dev_df.to_json('../data/facebook/dev.jsonl', orient = 'records')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HMC",
   "language": "python",
   "name": "hmc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
