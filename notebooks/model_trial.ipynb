{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-04-26T11:35:03.376443Z","iopub.status.busy":"2023-04-26T11:35:03.375891Z","iopub.status.idle":"2023-04-26T11:35:21.472876Z","shell.execute_reply":"2023-04-26T11:35:21.471538Z","shell.execute_reply.started":"2023-04-26T11:35:03.376385Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting ultralytics\n","  Downloading ultralytics-8.0.87-py3-none-any.whl (530 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m530.2/530.2 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: Pillow>=7.1.2 in /opt/conda/lib/python3.7/site-packages (from ultralytics) (9.4.0)\n","Requirement already satisfied: seaborn>=0.11.0 in /opt/conda/lib/python3.7/site-packages (from ultralytics) (0.12.2)\n","Requirement already satisfied: matplotlib>=3.2.2 in /opt/conda/lib/python3.7/site-packages (from ultralytics) (3.5.3)\n","Requirement already satisfied: torchvision>=0.8.1 in /opt/conda/lib/python3.7/site-packages (from ultralytics) (0.14.0)\n","Collecting thop>=0.1.1\n","  Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n","Requirement already satisfied: sentry-sdk in /opt/conda/lib/python3.7/site-packages (from ultralytics) (1.18.0)\n","Requirement already satisfied: requests>=2.23.0 in /opt/conda/lib/python3.7/site-packages (from ultralytics) (2.28.2)\n","Requirement already satisfied: psutil in /opt/conda/lib/python3.7/site-packages (from ultralytics) (5.9.3)\n","Requirement already satisfied: numpy>=1.21.6 in /opt/conda/lib/python3.7/site-packages (from ultralytics) (1.21.6)\n","Requirement already satisfied: scipy>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from ultralytics) (1.7.3)\n","Requirement already satisfied: PyYAML>=5.3.1 in /opt/conda/lib/python3.7/site-packages (from ultralytics) (6.0)\n","Requirement already satisfied: tqdm>=4.64.0 in /opt/conda/lib/python3.7/site-packages (from ultralytics) (4.64.1)\n","Requirement already satisfied: torch>=1.7.0 in /opt/conda/lib/python3.7/site-packages (from ultralytics) (1.13.0)\n","Collecting opencv-python>=4.6.0\n","  Downloading opencv_python-4.7.0.72-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (61.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.8/61.8 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas>=1.1.4 in /opt/conda/lib/python3.7/site-packages (from ultralytics) (1.3.5)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.2.2->ultralytics) (1.4.4)\n","Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.2.2->ultralytics) (2.8.2)\n","Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.2.2->ultralytics) (4.38.0)\n","Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.2.2->ultralytics) (3.0.9)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.2.2->ultralytics) (23.0)\n","Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.2.2->ultralytics) (0.11.0)\n","Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=1.1.4->ultralytics) (2023.3)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.23.0->ultralytics) (1.26.14)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.23.0->ultralytics) (3.4)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.23.0->ultralytics) (2.1.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.23.0->ultralytics) (2022.12.7)\n","Requirement already satisfied: typing_extensions in /opt/conda/lib/python3.7/site-packages (from seaborn>=0.11.0->ultralytics) (4.4.0)\n","Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib>=3.2.2->ultralytics) (1.16.0)\n","Installing collected packages: opencv-python, thop, ultralytics\n","  Attempting uninstall: opencv-python\n","    Found existing installation: opencv-python 4.5.4.60\n","    Uninstalling opencv-python-4.5.4.60:\n","      Successfully uninstalled opencv-python-4.5.4.60\n","Successfully installed opencv-python-4.7.0.72 thop-0.1.1.post2209072238 ultralytics-8.0.87\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["!pip install ultralytics"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-04-26T11:35:53.166944Z","iopub.status.busy":"2023-04-26T11:35:53.165765Z","iopub.status.idle":"2023-04-26T11:36:04.212928Z","shell.execute_reply":"2023-04-26T11:36:04.211699Z","shell.execute_reply.started":"2023-04-26T11:35:53.166899Z"},"trusted":true},"outputs":[],"source":["# Import the required libraries\n","import pandas as pd\n","from ultralytics import YOLO\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import torchvision.transforms as transforms\n","from torchvision import models\n","from transformers import BertTokenizer, VisualBertModel, logging\n","from PIL import Image\n","from tqdm import tqdm\n","import os\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","logging.set_verbosity_error()\n","import numpy as np"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-04-26T11:36:07.382747Z","iopub.status.busy":"2023-04-26T11:36:07.382332Z","iopub.status.idle":"2023-04-26T11:36:07.529364Z","shell.execute_reply":"2023-04-26T11:36:07.528172Z","shell.execute_reply.started":"2023-04-26T11:36:07.382707Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>img</th>\n","      <th>text</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>42953</td>\n","      <td>train/non_hateful/42953.png</td>\n","      <td>it their charact not their color that matter</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>23058</td>\n","      <td>train/non_hateful/23058.png</td>\n","      <td>dont be afraid to love again everyon is not li...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>13894</td>\n","      <td>train/non_hateful/13894.png</td>\n","      <td>put bow on your pet</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>37408</td>\n","      <td>train/non_hateful/37408.png</td>\n","      <td>i love everyth and everybodi except for squirr...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>82403</td>\n","      <td>train/non_hateful/82403.png</td>\n","      <td>everybodi love chocol chip cooki even hitler</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      id                          img  \\\n","0  42953  train/non_hateful/42953.png   \n","1  23058  train/non_hateful/23058.png   \n","2  13894  train/non_hateful/13894.png   \n","3  37408  train/non_hateful/37408.png   \n","4  82403  train/non_hateful/82403.png   \n","\n","                                                text  label  \n","0       it their charact not their color that matter      0  \n","1  dont be afraid to love again everyon is not li...      0  \n","2                                put bow on your pet      0  \n","3  i love everyth and everybodi except for squirr...      0  \n","4       everybodi love chocol chip cooki even hitler      0  "]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["# Read the dataframe containing the path to image, text and label\n","train_df = pd.read_json(\"../data/facebook/train.json\")\n","dev_df = pd.read_json(\"../data/facebook/dev.json\")\n","train_df.head()"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-04-26T11:36:08.961463Z","iopub.status.busy":"2023-04-26T11:36:08.960479Z","iopub.status.idle":"2023-04-26T11:36:09.059078Z","shell.execute_reply":"2023-04-26T11:36:09.057904Z","shell.execute_reply.started":"2023-04-26T11:36:08.961425Z"},"trusted":true},"outputs":[],"source":["# Declare some global variables\n","BATCH_SIZE = 64\n","EPOCHS = 5\n","ROOT_PATH = '../data/facebook'\n","IMAGE_SIZE = 224*224\n","NUM_CLASSES = 2\n","TEXTUAL_DIMENSION = 512\n","VISUAL_DIMENSION = 512\n","CHECKPOINT = './model.pt'\n","DEVICE = torch.device('cuda' if torch.cuda.is_available() else'cpu')"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-04-26T11:36:11.279691Z","iopub.status.busy":"2023-04-26T11:36:11.279243Z","iopub.status.idle":"2023-04-26T11:36:11.288154Z","shell.execute_reply":"2023-04-26T11:36:11.286734Z","shell.execute_reply.started":"2023-04-26T11:36:11.279653Z"},"trusted":true},"outputs":[],"source":["# Initialize the dataset and maintain the dataloader\n","class DynamicDataset(Dataset):\n","    def __init__(self, json_path, transform = None):\n","        self.df = pd.read_json(json_path)\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.df)\n","    \n","    def __getitem__(self, index):\n","        img_path = self.df.loc[index, 'img']\n","        img_file = os.path.join(ROOT_PATH, img_path)\n","        image = Image.open(img_file).convert(\"RGB\")\n","        if self.transform is not None:\n","            image = self.transform(image)\n","        \n","        text = self.df.loc[index, 'text']\n","        if 'label' not in self.df.columns:\n","            return image, text\n","        label = self.df.loc[index, 'label']\n","\n","        return image ,text, label"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-04-26T11:36:13.683156Z","iopub.status.busy":"2023-04-26T11:36:13.682428Z","iopub.status.idle":"2023-04-26T11:36:13.759117Z","shell.execute_reply":"2023-04-26T11:36:13.758009Z","shell.execute_reply.started":"2023-04-26T11:36:13.683116Z"},"trusted":true},"outputs":[],"source":["# Define a transform function for image preprocessing\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","])\n","\n","# Create objects of each set of data\n","train_data = DynamicDataset(os.path.join(ROOT_PATH, 'train.json'), transform = transform)\n","dev_data = DynamicDataset(os.path.join(ROOT_PATH, 'dev.json'), transform = transform)\n","\n","# Create a dataloader\n","train_loader = DataLoader(train_data, batch_size = BATCH_SIZE, shuffle = True)\n","dev_loader = DataLoader(dev_data, batch_size = BATCH_SIZE, shuffle = True)"]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.execute_input":"2023-04-26T11:57:33.344280Z","iopub.status.busy":"2023-04-26T11:57:33.343537Z","iopub.status.idle":"2023-04-26T11:57:33.355149Z","shell.execute_reply":"2023-04-26T11:57:33.354077Z","shell.execute_reply.started":"2023-04-26T11:57:33.344242Z"},"trusted":true},"outputs":[],"source":["# Construct a class for extracting visual features from resnet50 architecture\n","class Visual_Feature(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","        # Define resnet50 model\n","        resnet50 = YOLO('yolov8n.pt')\n","        convolution_layers = nn.Sequential(\n","            nn.Conv2d(2048, 1024, kernel_size=(3, 3), stride = (1, 1), padding = (1, 1)),\n","            nn.ReLU(),\n","            nn.Conv2d(1024, 512, kernel_size=(3, 3), stride = (1, 1), padding = (1, 1)),\n","            nn.ReLU(),\n","        )\n","        \n","#         # Freeze parameters\n","#         for param in resnet50.parameters():\n","#             param.requires_grad = False\n","\n","#         self.resnet50 = nn.Sequential(*list(resnet50.children())[:-1])\n","        self.resnet50 = resnet50\n","        self.convolution_layers = convolution_layers\n","\n","    def get_visual_features(self, images, get_conv_features):\n","        # Extract visual features from resnet50 model\n","        \n","        \"\"\"\n","            Based on the value of get_conv_features, convolution layers are applied.\n","            This is required because visual bert requires the input visual fetures to be 2048.\n","            ResNet50 by default gives 2048 features and hence there is no need to apply conv. layers.\n","        \"\"\"\n","        \n","        # Define the scope of the variable\n","        visual_features = None\n","        if(get_conv_features):\n","            visual_features = self.convolution_layers(self.resnet50(images))\n","        else:\n","            visual_features = self.resnet50.predict(images)\n","#         visual_features = visual_features.reshape(images.shape[0], 1, -1)\n","\n","        return visual_features"]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2023-04-26T11:57:59.935236Z","iopub.status.busy":"2023-04-26T11:57:59.934645Z","iopub.status.idle":"2023-04-26T11:58:00.396330Z","shell.execute_reply":"2023-04-26T11:58:00.394424Z","shell.execute_reply.started":"2023-04-26T11:57:59.935191Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\n","13.8ms\n"]},{"name":"stdout","output_type":"stream","text":["[ultralytics.yolo.engine.results.Results object with attributes:\n","\n","boxes: ultralytics.yolo.engine.results.Boxes object\n","keypoints: None\n","keys: ['boxes']\n","masks: None\n","names: {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n","orig_img: tensor([[[[ 0.0062,  0.0055,  0.0007,  ...,  0.0057, -0.0036, -0.0064],\n","          [ 0.0056,  0.0051,  0.0049,  ...,  0.0059, -0.0033, -0.0064],\n","          [ 0.0055,  0.0049,  0.0050,  ...,  0.0059, -0.0030, -0.0064],\n","          ...,\n","          [ 0.0088,  0.0088,  0.0088,  ...,  0.0088,  0.0088,  0.0088],\n","          [ 0.0088,  0.0088,  0.0088,  ...,  0.0088,  0.0088,  0.0088],\n","          [ 0.0088,  0.0088,  0.0088,  ...,  0.0088,  0.0088,  0.0088]],\n","\n","         [[ 0.0068,  0.0062,  0.0012,  ...,  0.0064, -0.0032, -0.0061],\n","          [ 0.0062,  0.0057,  0.0055,  ...,  0.0065, -0.0029, -0.0061],\n","          [ 0.0062,  0.0055,  0.0056,  ...,  0.0066, -0.0026, -0.0061],\n","          ...,\n","          [ 0.0095,  0.0095,  0.0095,  ...,  0.0095,  0.0095,  0.0095],\n","          [ 0.0095,  0.0095,  0.0095,  ...,  0.0095,  0.0095,  0.0095],\n","          [ 0.0095,  0.0095,  0.0095,  ...,  0.0095,  0.0095,  0.0095]],\n","\n","         [[ 0.0077,  0.0070,  0.0021,  ...,  0.0072, -0.0023, -0.0052],\n","          [ 0.0071,  0.0066,  0.0063,  ...,  0.0073, -0.0020, -0.0052],\n","          [ 0.0070,  0.0064,  0.0065,  ...,  0.0074, -0.0017, -0.0052],\n","          ...,\n","          [ 0.0104,  0.0104,  0.0104,  ...,  0.0104,  0.0104,  0.0104],\n","          [ 0.0104,  0.0104,  0.0104,  ...,  0.0104,  0.0104,  0.0104],\n","          [ 0.0104,  0.0104,  0.0104,  ...,  0.0104,  0.0104,  0.0104]]]], device='cuda:0')\n","orig_shape: torch.Size([1, 3])\n","path: None\n","probs: None\n","speed: {'preprocess': 0.48065185546875, 'inference': 13.791799545288086, 'postprocess': 0.3428459167480469}]\n"]}],"source":["# Test yolov8 (WORKS BUT SKIPPED TO PRESERVE MEMORY)\n","resnet50 = Visual_Feature()\n","resnet50.to(DEVICE)\n","image = Image.open(os.path.join(ROOT_PATH, 'dev/hateful/01456.png'))\n","image = transform(image).reshape(1, 3, 224, 224)\n","visual_features = resnet50.get_visual_features(image.to(DEVICE), get_conv_features = False)\n","print(visual_features)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-04-26T11:39:59.605653Z","iopub.status.idle":"2023-04-26T11:39:59.606013Z","shell.execute_reply":"2023-04-26T11:39:59.605860Z","shell.execute_reply.started":"2023-04-26T11:39:59.605842Z"},"trusted":true},"outputs":[],"source":["class Textual_Feature(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","        # Define virtual bert model\n","        visual_bert = VisualBertModel.from_pretrained('uclanlp/visualbert-vqa')\n","        dense_layers = nn.Sequential(            \n","            nn.Linear(768, 512), # 768 is the dimensions returned by the visual bert model\n","            nn.ReLU(),\n","        )\n","\n","        self.visual_bert = visual_bert\n","        self.dense_layers = dense_layers\n","\n","        # Define tokenizer\n","        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","    \n","    def get_textual_features(self, images, texts):\n","        # Define indices and attention mask\n","        inputs = self.tokenizer(texts, padding = True, truncation = True, return_tensors = 'pt').to(DEVICE)\n","        input_ids = inputs['input_ids']\n","        attention_mask = inputs['attention_mask']\n","        token_ids = inputs['token_type_ids']\n","\n","        # Extract visual features\n","        resnet50 = Visual_Feature().to(DEVICE)\n","        visual_features = resnet50.get_visual_features(images.to(DEVICE), get_conv_features = False)\n","        visual_token_ids = torch.ones(visual_features.shape[:-1], dtype=torch.long).to(DEVICE)\n","            \n","        visual_attention_mask = torch.ones(visual_features.shape[:-1], dtype=torch.float).to(DEVICE)\n","            \n","        # Extract textual features from virtual bert model\n","        textual_features = self.visual_bert(\n","            input_ids = input_ids,\n","            attention_mask = attention_mask,\n","            token_type_ids = token_ids,\n","            visual_embeds = visual_features, # pass the visual features as received from resnet50\n","            visual_token_type_ids = visual_token_ids,\n","            visual_attention_mask = visual_attention_mask,\n","        )\n","        \n","        textual_features = textual_features[0][:, 0, :] # Extract the first token of last hidden state\n","            \n","        textual_features = self.dense_layers(textual_features)\n","        return textual_features"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-04-26T11:38:20.500377Z","iopub.status.busy":"2023-04-26T11:38:20.499624Z","iopub.status.idle":"2023-04-26T11:38:20.504965Z","shell.execute_reply":"2023-04-26T11:38:20.503702Z","shell.execute_reply.started":"2023-04-26T11:38:20.500337Z"},"trusted":true},"outputs":[],"source":["# # Test visual bert (WORKS BUT SKIPPED TO PRESERVE MEMORY)\n","# vbert = Textual_Feature().to(DEVICE)\n","# for images, texts, labels in tqdm(train_loader):\n","#     images = images.to(DEVICE)\n","#     textual_feature = vbert.get_textual_features(images, texts)\n","#     print(textual_feature.shape)\n","#     break"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-21T14:36:48.671734Z","iopub.status.busy":"2023-04-21T14:36:48.671422Z","iopub.status.idle":"2023-04-21T14:36:48.682141Z","shell.execute_reply":"2023-04-21T14:36:48.681087Z","shell.execute_reply.started":"2023-04-21T14:36:48.671708Z"},"trusted":true},"outputs":[],"source":["class Fusion(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","        # Define fusion layers\n","        fusion_layers = nn.Sequential(\n","            nn.Linear((VISUAL_DIMENSION + TEXTUAL_DIMENSION), 512),\n","            nn.ReLU(),\n","            nn.Linear(512, 256),\n","            nn.ReLU(),\n","            nn.Linear(256, 128),\n","            nn.ReLU(),\n","            nn.Linear(128, 64),\n","            nn.ReLU(),\n","            nn.Linear(64, 32),\n","            nn.ReLU(),\n","            nn.Linear(32, 1),\n","        )\n","\n","        self.fusion_layers = fusion_layers\n","    \n","    def forward(self, images, texts):\n","        # Initialize text and visual classes\n","        visual_class = Visual_Feature().to(DEVICE)\n","        textual_class = Textual_Feature().to(DEVICE)\n","\n","        # Extract visual and textual features\n","        visual_features = visual_class.get_visual_features(images, get_conv_features = True).reshape(images.shape[0], -1)\n","        \n","        textual_features = textual_class.get_textual_features(images, texts)\n","\n","        # Concatenate visual and textual features\n","        features = torch.cat((visual_features, textual_features), dim = 1)\n","\n","        # Pass through fusion layers\n","        output = self.fusion_layers(features)\n","\n","        return output"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-21T14:36:48.684245Z","iopub.status.busy":"2023-04-21T14:36:48.683772Z","iopub.status.idle":"2023-04-21T14:36:51.975573Z","shell.execute_reply":"2023-04-21T14:36:51.974352Z","shell.execute_reply.started":"2023-04-21T14:36:48.684210Z"},"trusted":true},"outputs":[],"source":["# Define fusion model\n","fusion = Fusion().to(DEVICE)\n","print(fusion)\n","\n","# Define loss function and optimizer\n","criterion = nn.BCEWithLogitsLoss()\n","\n","# Define optimizer\n","optimizer = optim.Adam(fusion.parameters(), lr = 0.001)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-21T14:36:51.977751Z","iopub.status.busy":"2023-04-21T14:36:51.977356Z","iopub.status.idle":"2023-04-21T14:36:51.987008Z","shell.execute_reply":"2023-04-21T14:36:51.985973Z","shell.execute_reply.started":"2023-04-21T14:36:51.977712Z"},"trusted":true},"outputs":[],"source":["def train_model(model):\n","    # Initialize required variables\n","    train_loss = 0\n","    train_acc = 0\n","    total = 0\n","    correct = 0\n","    \n","    # Loop over all the batches\n","    for images, texts, labels in tqdm(train_loader):\n","        images = images.to(DEVICE)\n","        labels = torch.reshape(labels, (-1, 1)).to(dtype = torch.float32, device = DEVICE)\n","\n","        optimizer.zero_grad(set_to_none = True) # Uses less memory\n","        outputs = fusion(images, texts)\n","\n","        predicted = torch.round(torch.sigmoid(outputs))\n","        \n","        total += labels.size(0) # Must be adding equivalent to batch size\n","        correct += (predicted == labels).sum().item()\n","\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        train_loss += loss.item() * images.size(0)\n","    train_acc = 100 * correct / total\n","    train_loss /= len(train_data)\n","    return train_acc, train_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-21T14:36:51.989107Z","iopub.status.busy":"2023-04-21T14:36:51.988792Z","iopub.status.idle":"2023-04-21T14:36:51.998974Z","shell.execute_reply":"2023-04-21T14:36:51.998046Z","shell.execute_reply.started":"2023-04-21T14:36:51.989081Z"},"trusted":true},"outputs":[],"source":["def eval_model(model):    \n","    # Initialize the required variables\n","    dev_loss = 0\n","    dev_acc = 0\n","    total = 0\n","    correct = 0\n","    \n","    for images, texts, labels in tqdm(dev_loader):\n","        images = images.to(DEVICE)\n","        labels = torch.reshape(labels, (-1, 1)).to(dtype = torch.float32, device = DEVICE)\n","        \n","        outputs = model(images, texts)\n","        predicted = torch.round(torch.sigmoid(outputs)) # threshold issues\n","        \n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","        \n","        loss = criterion(outputs, labels)\n","        dev_loss += loss.item() * images.size(0)\n","        \n","    dev_acc = 100 * correct / total\n","    dev_loss /= len(dev_data)\n","    \n","    return dev_acc, dev_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-21T14:36:52.001070Z","iopub.status.busy":"2023-04-21T14:36:52.000652Z","iopub.status.idle":"2023-04-21T14:36:52.011391Z","shell.execute_reply":"2023-04-21T14:36:52.010315Z","shell.execute_reply.started":"2023-04-21T14:36:52.001034Z"},"trusted":true},"outputs":[],"source":["def save_model(prev_acc, curr_acc, epoch, model, optimizer):\n","    # Compare and save\n","    if curr_acc >= prev_acc:\n","        # Save the model\n","        torch.save({\n","            'epoch': epoch,\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","        }, CHECKPOINT)\n","        \n","        # Return new highest accuracy\n","        return curr_acc\n","    return prev_acc"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-21T14:36:52.013433Z","iopub.status.busy":"2023-04-21T14:36:52.013059Z","iopub.status.idle":"2023-04-21T16:34:04.912894Z","shell.execute_reply":"2023-04-21T16:34:04.911803Z","shell.execute_reply.started":"2023-04-21T14:36:52.013399Z"},"trusted":true},"outputs":[],"source":["prev_dev_acc = 0\n","dev_acc = 0\n","try:\n","    for epoch in range(EPOCHS):\n","        # Train model\n","        fusion = fusion.train()\n","        train_acc, train_loss = train_model(fusion)\n","        print(f\"Epoch {epoch+1}/{EPOCHS}, Train Loss = {train_loss:.4f}, Train Accuracy = {train_acc:.4f}\")\n","\n","        # Evaluate model\n","        fusion.eval()\n","        dev_acc, dev_loss = eval_model(fusion)\n","        print(f\"Epoch {epoch+1}/{EPOCHS}, Dev Loss = {dev_loss:.4f}, Dev Accuracy = {dev_acc:.4f}\")\n","\n","        # Save best model\n","        prev_dev_acc = save_model(prev_dev_acc, dev_acc, epoch + 1, fusion, optimizer)\n","    \n","except Exception as e:\n","    # Log the exception\n","    print(e)\n","\n","    # Save best model\n","    prev_dev_acc = save_model(prev_dev_acc, dev_acc, epoch, fusion, optimizer)"]}],"metadata":{"kernelspec":{"display_name":"hmc","language":"python","name":"venv"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":4}
