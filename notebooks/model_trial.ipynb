{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-04-21T08:02:32.006408Z","iopub.status.busy":"2023-04-21T08:02:32.005328Z","iopub.status.idle":"2023-04-21T08:02:34.992590Z","shell.execute_reply":"2023-04-21T08:02:34.991009Z","shell.execute_reply.started":"2023-04-21T08:02:32.006360Z"},"trusted":true},"outputs":[],"source":["# Import the required libraries\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import torchvision.transforms as transforms\n","from torchvision import models\n","from transformers import BertTokenizer, VisualBertModel, VisualBertForPreTraining, logging\n","from PIL import Image\n","from tqdm import tqdm\n","import os\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","logging.set_verbosity_error()\n","import numpy as np"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-04-21T08:02:34.996889Z","iopub.status.busy":"2023-04-21T08:02:34.996321Z","iopub.status.idle":"2023-04-21T08:02:35.094347Z","shell.execute_reply":"2023-04-21T08:02:35.093222Z","shell.execute_reply.started":"2023-04-21T08:02:34.996852Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>img</th>\n","      <th>text</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>42953</td>\n","      <td>train/non_hateful/42953.png</td>\n","      <td>it their charact not their color that matter</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>23058</td>\n","      <td>train/non_hateful/23058.png</td>\n","      <td>dont be afraid to love again everyon is not li...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>13894</td>\n","      <td>train/non_hateful/13894.png</td>\n","      <td>put bow on your pet</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>37408</td>\n","      <td>train/non_hateful/37408.png</td>\n","      <td>i love everyth and everybodi except for squirr...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>82403</td>\n","      <td>train/non_hateful/82403.png</td>\n","      <td>everybodi love chocol chip cooki even hitler</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      id                          img  \\\n","0  42953  train/non_hateful/42953.png   \n","1  23058  train/non_hateful/23058.png   \n","2  13894  train/non_hateful/13894.png   \n","3  37408  train/non_hateful/37408.png   \n","4  82403  train/non_hateful/82403.png   \n","\n","                                                text  label  \n","0       it their charact not their color that matter      0  \n","1  dont be afraid to love again everyon is not li...      0  \n","2                                put bow on your pet      0  \n","3  i love everyth and everybodi except for squirr...      0  \n","4       everybodi love chocol chip cooki even hitler      0  "]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["# Read the dataframe containing the path to image, text and label\n","train_df = pd.read_json(\"/kaggle/input/facebook-hmc/facebook/train.json\")\n","dev_df = pd.read_json(\"/kaggle/input/facebook-hmc/facebook/dev.json\")\n","train_df.head()"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-04-21T08:02:35.098646Z","iopub.status.busy":"2023-04-21T08:02:35.097980Z","iopub.status.idle":"2023-04-21T08:02:35.184365Z","shell.execute_reply":"2023-04-21T08:02:35.183077Z","shell.execute_reply.started":"2023-04-21T08:02:35.098612Z"},"trusted":true},"outputs":[],"source":["# Declare some global variables\n","BATCH_SIZE = 32\n","EPOCHS = 5\n","ROOT_PATH = '/kaggle/input/facebook-hmc/facebook'\n","IMAGE_SIZE = 224*224\n","NUM_CLASSES = 2\n","TEXTUAL_DIMENSION = 512\n","VISUAL_DIMENSION = 512\n","CHECKPOINT = '/kaggle/working/model.pt'\n","DEVICE = torch.device('cuda' if torch.cuda.is_available() else'cpu')"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-04-21T08:02:35.188651Z","iopub.status.busy":"2023-04-21T08:02:35.188347Z","iopub.status.idle":"2023-04-21T08:02:35.199265Z","shell.execute_reply":"2023-04-21T08:02:35.198216Z","shell.execute_reply.started":"2023-04-21T08:02:35.188622Z"},"trusted":true},"outputs":[],"source":["# Initialize the dataset and maintain the dataloader\n","class DynamicDataset(Dataset):\n","    def __init__(self, json_path, transform = None):\n","        self.df = pd.read_json(json_path)\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.df)\n","    \n","    def __getitem__(self, index):\n","        img_path = self.df.loc[index, 'img']\n","        img_file = os.path.join(ROOT_PATH, img_path)\n","        image = Image.open(img_file).convert(\"RGB\")\n","        if self.transform is not None:\n","            image = self.transform(image)\n","        \n","        text = self.df.loc[index, 'text']\n","        if 'label' not in self.df.columns:\n","            return image, text\n","        label = self.df.loc[index, 'label']\n","\n","        return image ,text, label"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-04-21T08:02:35.203201Z","iopub.status.busy":"2023-04-21T08:02:35.202830Z","iopub.status.idle":"2023-04-21T08:02:35.276857Z","shell.execute_reply":"2023-04-21T08:02:35.275833Z","shell.execute_reply.started":"2023-04-21T08:02:35.203172Z"},"trusted":true},"outputs":[],"source":["# Define a transform function for image preprocessing\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","])\n","\n","# Create objects of each set of data\n","train_data = DynamicDataset(os.path.join(ROOT_PATH, 'train.json'), transform = transform)\n","dev_data = DynamicDataset(os.path.join(ROOT_PATH, 'dev.json'), transform = transform)\n","\n","# Create a dataloader\n","train_loader = DataLoader(train_data, batch_size = BATCH_SIZE, shuffle = True)\n","dev_loader = DataLoader(dev_data, batch_size = BATCH_SIZE, shuffle = True)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-04-21T08:02:35.280616Z","iopub.status.busy":"2023-04-21T08:02:35.280325Z","iopub.status.idle":"2023-04-21T08:02:35.292403Z","shell.execute_reply":"2023-04-21T08:02:35.291214Z","shell.execute_reply.started":"2023-04-21T08:02:35.280588Z"},"trusted":true},"outputs":[],"source":["# Construct a class for extracting visual features from resnet50 architecture\n","class Visual_Feature(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","        # Define resnet50 model\n","        resnet50 = models.resnet50(weights = models.ResNet50_Weights.DEFAULT)\n","        convolution_layers = nn.Sequential(\n","            nn.Conv2d(2048, 1024, kernel_size=(3, 3), stride = (1, 1), padding = (1, 1)),\n","            nn.ReLU(),\n","            nn.Conv2d(1024, 512, kernel_size=(3, 3), stride = (1, 1), padding = (1, 1)),\n","            nn.ReLU(),\n","        )\n","        \n","        # Freeze parameters\n","        for param in resnet50.parameters():\n","            param.requires_grad = False\n","\n","        self.resnet50 = nn.Sequential(*list(resnet50.children())[:-1])\n","        self.convolution_layers = convolution_layers\n","\n","    def get_visual_features(self, images, get_conv_features):\n","        # Extract visual features from resnet50 model\n","        \n","        \"\"\"\n","            Based on the value of get_conv_features, convolution layers are applied.\n","            This is required because visual bert requires the input visual fetures to be 2048.\n","            ResNet50 by default gives 2048 features and hence there is no need to apply conv. layers.\n","        \"\"\"\n","        \n","        # Define the scope of the variable\n","        visual_features = None\n","        if(get_conv_features):\n","            visual_features = self.convolution_layers(self.resnet50(images))\n","        else:\n","            visual_features = self.resnet50(images)\n","        visual_features = visual_features.reshape(BATCH_SIZE, 1, -1)\n","\n","        return visual_features"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-04-21T08:02:35.296217Z","iopub.status.busy":"2023-04-21T08:02:35.295353Z","iopub.status.idle":"2023-04-21T08:02:35.309223Z","shell.execute_reply":"2023-04-21T08:02:35.308195Z","shell.execute_reply.started":"2023-04-21T08:02:35.296174Z"},"trusted":true},"outputs":[],"source":["class Textual_Feature(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","        # Define virtual bert model\n","        visual_bert = VisualBertModel.from_pretrained('uclanlp/visualbert-vqa')\n","        dense_layers = nn.Sequential(\n","#             nn.Linear(30522, 20000), # 30522 is the dimensions returned by the visual bert pretrained model\n","#             nn.ReLU(),\n","#             nn.Linear(20000, 10000),\n","#             nn.ReLU(),\n","#             nn.Linear(10000, 5000),\n","#             nn.ReLU(),\n","#             nn.Linear(5000, 2000),\n","#             nn.ReLU(),\n","#             nn.Linear(2000, 1000),\n","#             nn.ReLU(),\n","#             nn.Linear(1000, 512),\n","#             nn.ReLU(),\n","            \n","            nn.Linear(768, 512),\n","            nn.ReLU(),\n","        )\n","        \n","        # Freeze parameters\n","#         for param in visual_bert.parameters():\n","#             param.requires_grad = False\n","\n","        self.visual_bert = visual_bert\n","        self.dense_layers = dense_layers\n","\n","        # Define tokenizer\n","        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","    \n","    def get_textual_features(self, images, texts):\n","        # Define indices and attention mask\n","        inputs = self.tokenizer(texts, padding = True, truncation = True, return_tensors = 'pt').to(DEVICE)\n","        input_ids = inputs['input_ids']\n","        attention_mask = inputs['attention_mask']\n","        token_ids = inputs['token_type_ids']\n","\n","        # Extract visual features\n","        resnet50 = Visual_Feature().to(DEVICE)\n","        visual_features = resnet50.get_visual_features(images.to(DEVICE), get_conv_features = False)\n","        visual_token_ids = torch.ones(visual_features.shape[:-1], dtype=torch.long).to(DEVICE)\n","        visual_attention_mask = torch.ones(visual_features.shape[:-1], dtype=torch.float).to(DEVICE)\n","\n","        # Extract textual features from virtual bert model\n","        textual_features = self.visual_bert(\n","            input_ids = input_ids,\n","            attention_mask = attention_mask,\n","            token_type_ids = token_ids,\n","            visual_embeds = visual_features, # pass the visual features as received from resnet50\n","            visual_token_type_ids = visual_token_ids,\n","            visual_attention_mask = visual_attention_mask,\n","        )\n","        \n","        textual_features = textual_features[0][:, 0, :] # Extract the first token of last hidden state\n","        textual_features = self.dense_layers(textual_features)\n","\n","        return textual_features"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-04-21T08:02:35.311513Z","iopub.status.busy":"2023-04-21T08:02:35.310812Z","iopub.status.idle":"2023-04-21T08:02:35.323152Z","shell.execute_reply":"2023-04-21T08:02:35.321974Z","shell.execute_reply.started":"2023-04-21T08:02:35.311471Z"},"trusted":true},"outputs":[],"source":["# Test visual bert (WORKS BUT SKIPPED TO PRESERVE MEMORY)\n","# vbert = Textual_Feature().to(DEVICE)\n","# for images, texts, labels in tqdm(train_loader):\n","#     images = images.to(DEVICE)\n","#     textual_feature = vbert.get_textual_features(images, texts)\n","#     print(textual_feature.shape)\n","#     break"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-04-21T08:02:35.325389Z","iopub.status.busy":"2023-04-21T08:02:35.324991Z","iopub.status.idle":"2023-04-21T08:02:35.337080Z","shell.execute_reply":"2023-04-21T08:02:35.335961Z","shell.execute_reply.started":"2023-04-21T08:02:35.325348Z"},"trusted":true},"outputs":[],"source":["# Test resnet50 (WORKS BUT SKIPPED TO PRESERVE MEMORY)\n","# resnet50 = Visual_Feature()\n","# resnet50.to(DEVICE)\n","# image = Image.open(os.path.join(ROOT_PATH, 'dev/hateful/01456.png'))\n","# image = transform(image).reshape(1, 3, 224, 224)\n","# visual_features = resnet50.get_visual_features(image.to(DEVICE), get_conv_features = True)\n","# print(visual_features.shape)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-04-21T08:02:35.340302Z","iopub.status.busy":"2023-04-21T08:02:35.339852Z","iopub.status.idle":"2023-04-21T08:02:35.350981Z","shell.execute_reply":"2023-04-21T08:02:35.349650Z","shell.execute_reply.started":"2023-04-21T08:02:35.340261Z"},"trusted":true},"outputs":[],"source":["class Fusion(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","        # Define fusion layers\n","        fusion_layers = nn.Sequential(\n","            nn.Linear((VISUAL_DIMENSION + TEXTUAL_DIMENSION), 512),\n","            nn.ReLU(),\n","            nn.Linear(512, 256),\n","            nn.ReLU(),\n","            nn.Linear(256, 128),\n","            nn.ReLU(),\n","            nn.Linear(128, 64),\n","            nn.ReLU(),\n","            nn.Linear(64, 32),\n","            nn.ReLU(),\n","            nn.Linear(32, 1),\n","        )\n","\n","        self.fusion_layers = fusion_layers\n","    \n","    def forward(self, images, texts):\n","        # Initialize text and visual classes\n","        visual_class = Visual_Feature().to(DEVICE)\n","        textual_class = Textual_Feature().to(DEVICE)\n","\n","        # Extract visual and textual features\n","        visual_features = visual_class.get_visual_features(images, get_conv_features = True).reshape(BATCH_SIZE, -1)\n","        \n","        textual_features = textual_class.get_textual_features(images, texts)\n","\n","        # Concatenate visual and textual features\n","        features = torch.cat((visual_features, textual_features), dim = 1)\n","\n","        # Pass through fusion layers\n","        output = self.fusion_layers(features)\n","\n","        return output"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-04-21T08:02:35.353332Z","iopub.status.busy":"2023-04-21T08:02:35.352818Z","iopub.status.idle":"2023-04-21T08:02:37.181426Z","shell.execute_reply":"2023-04-21T08:02:37.179348Z","shell.execute_reply.started":"2023-04-21T08:02:35.353185Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Fusion(\n","  (fusion_layers): Sequential(\n","    (0): Linear(in_features=1024, out_features=512, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=512, out_features=256, bias=True)\n","    (3): ReLU()\n","    (4): Linear(in_features=256, out_features=128, bias=True)\n","    (5): ReLU()\n","    (6): Linear(in_features=128, out_features=64, bias=True)\n","    (7): ReLU()\n","    (8): Linear(in_features=64, out_features=32, bias=True)\n","    (9): ReLU()\n","    (10): Linear(in_features=32, out_features=1, bias=True)\n","  )\n",")\n"]}],"source":["# Define fusion model\n","fusion = Fusion().to(DEVICE)\n","print(fusion)\n","\n","# Define loss function and optimizer\n","criterion = nn.BCEWithLogitsLoss()\n","\n","# Define optimizer\n","optimizer = optim.Adam(fusion.parameters(), lr = 0.001)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-04-21T08:02:37.183662Z","iopub.status.busy":"2023-04-21T08:02:37.183270Z","iopub.status.idle":"2023-04-21T08:02:37.194658Z","shell.execute_reply":"2023-04-21T08:02:37.193615Z","shell.execute_reply.started":"2023-04-21T08:02:37.183622Z"},"trusted":true},"outputs":[],"source":["def train_model(model):\n","    # Initialize required variables\n","    train_loss = 0\n","    train_acc = 0\n","    total = 0\n","    correct = 0\n","    \n","    # Loop over all the batches\n","    for images, texts, labels in tqdm(train_loader):\n","        images = images.to(DEVICE)\n","        labels = torch.reshape(labels, (-1, 1)).to(dtype = torch.float32, device = DEVICE)\n","\n","        optimizer.zero_grad(set_to_none = True) # Uses less memory\n","        outputs = fusion(images, texts)\n","\n","        predicted = torch.sigmoid(outputs)\n","        \n","        # For debugging purposes only\n","        print(outputs.reshape(1, -1))\n","        print(predicted.reshape(1, -1))\n","        predicted = torch.round(predicted)\n","        print(predicted.reshape(1, -1))\n","        print((predicted == labels).sum().item())\n","        print(labels.sum().item())\n","        \n","        total += labels.size(0) # Must be adding equivalent to batch size\n","        correct += (predicted == labels).sum().item()\n","\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        train_loss += loss.item() * images.size(0)\n","        train_acc = 100 * correct / total\n","        print(f\"Train Accuracy = {train_acc:.4f}\")\n","        # For debugging purposes only\n","    train_acc = 100 * correct / total\n","    train_loss /= len(train_data)\n","    return train_acc, train_loss\n","        "]},{"cell_type":"markdown","metadata":{},"source":["# Debugging Insights\n","* The outputs are almost same to each other and similar is the case for predicted variables. This means that the model is not able to train well."]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-04-21T08:02:37.199396Z","iopub.status.busy":"2023-04-21T08:02:37.199102Z","iopub.status.idle":"2023-04-21T08:02:37.209586Z","shell.execute_reply":"2023-04-21T08:02:37.208596Z","shell.execute_reply.started":"2023-04-21T08:02:37.199352Z"},"trusted":true},"outputs":[],"source":["# NOT CHECKED\n","def eval_model(model):    \n","    # Initialize the required variables\n","    dev_loss = 0\n","    dev_acc = 0\n","    total = 0\n","    correct = 0\n","    \n","    for images, texts, labels in tqdm(dev_loader):\n","        images = images.to(DEVICE)\n","        labels = torch.reshape(labels, (-1, 1)).to(dtype = torch.float32, device = DEVICE)\n","        \n","        outputs = model(images, texts)\n","        predicted = torch.round(torch.sigmoid(outputs)) # threshold issues\n","        \n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","        \n","        loss = criterion(outputs, labels)\n","        dev_loss += loss.item() * images.size(0)\n","        \n","    dev_acc = 100 * correct / total\n","    dev_loss /= len(dev_data)\n","    \n","    return dev_acc, dev_loss"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-04-21T08:02:37.212088Z","iopub.status.busy":"2023-04-21T08:02:37.210915Z","iopub.status.idle":"2023-04-21T08:02:37.226842Z","shell.execute_reply":"2023-04-21T08:02:37.225654Z","shell.execute_reply.started":"2023-04-21T08:02:37.212048Z"},"trusted":true},"outputs":[],"source":["# NOT CHECKED\n","def save_model(prev_acc, curr_acc, epoch, model, optimizer):\n","    # Compare and save\n","    if curr_acc > prev_acc:\n","        # Save the model\n","        torch.save({\n","            'epoch': epoch,\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","        }, CHECKPOINT)\n","        \n","        # Return new highest accuracy\n","        return curr_acc\n","    return prev_acc"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-04-21T08:02:37.228862Z","iopub.status.busy":"2023-04-21T08:02:37.228484Z","iopub.status.idle":"2023-04-21T08:03:00.144757Z","shell.execute_reply":"2023-04-21T08:03:00.143077Z","shell.execute_reply.started":"2023-04-21T08:02:37.228823Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/682 [00:00<?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["tensor([[-0.1631, -0.1628, -0.1635, -0.1635, -0.1635, -0.1627, -0.1643, -0.1636,\n","         -0.1651, -0.1626, -0.1630, -0.1639, -0.1642, -0.1642, -0.1633, -0.1639,\n","         -0.1645, -0.1641, -0.1631, -0.1648, -0.1638, -0.1647, -0.1647, -0.1656,\n","         -0.1643, -0.1650, -0.1667, -0.1644, -0.1637, -0.1629, -0.1649, -0.1625]],\n","       device='cuda:0', grad_fn=<ReshapeAliasBackward0>)\n","tensor([[0.4593, 0.4594, 0.4592, 0.4592, 0.4592, 0.4594, 0.4590, 0.4592, 0.4588,\n","         0.4594, 0.4593, 0.4591, 0.4590, 0.4590, 0.4593, 0.4591, 0.4590, 0.4591,\n","         0.4593, 0.4589, 0.4591, 0.4589, 0.4589, 0.4587, 0.4590, 0.4588, 0.4584,\n","         0.4590, 0.4592, 0.4594, 0.4589, 0.4595]], device='cuda:0',\n","       grad_fn=<ReshapeAliasBackward0>)\n","tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0',\n","       grad_fn=<ReshapeAliasBackward0>)\n","15\n","17.0\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 1/682 [00:05<1:06:26,  5.85s/it]"]},{"name":"stdout","output_type":"stream","text":["Train Accuracy = 46.8750\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 2/682 [00:09<54:31,  4.81s/it]  "]},{"name":"stdout","output_type":"stream","text":["tensor([[-0.1532, -0.1520, -0.1534, -0.1530, -0.1533, -0.1520, -0.1532, -0.1523,\n","         -0.1525, -0.1531, -0.1520, -0.1522, -0.1521, -0.1520, -0.1527, -0.1526,\n","         -0.1530, -0.1528, -0.1525, -0.1527, -0.1530, -0.1524, -0.1530, -0.1523,\n","         -0.1526, -0.1518, -0.1540, -0.1529, -0.1521, -0.1534, -0.1518, -0.1532]],\n","       device='cuda:0', grad_fn=<ReshapeAliasBackward0>)\n","tensor([[0.4618, 0.4621, 0.4617, 0.4618, 0.4617, 0.4621, 0.4618, 0.4620, 0.4619,\n","         0.4618, 0.4621, 0.4620, 0.4621, 0.4621, 0.4619, 0.4619, 0.4618, 0.4619,\n","         0.4619, 0.4619, 0.4618, 0.4620, 0.4618, 0.4620, 0.4619, 0.4621, 0.4616,\n","         0.4619, 0.4621, 0.4617, 0.4621, 0.4618]], device='cuda:0',\n","       grad_fn=<ReshapeAliasBackward0>)\n","tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0',\n","       grad_fn=<ReshapeAliasBackward0>)\n","13\n","19.0\n","Train Accuracy = 43.7500\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 3/682 [00:13<48:21,  4.27s/it]"]},{"name":"stdout","output_type":"stream","text":["tensor([[-0.1417, -0.1418, -0.1425, -0.1413, -0.1426, -0.1423, -0.1412, -0.1428,\n","         -0.1423, -0.1433, -0.1435, -0.1418, -0.1422, -0.1414, -0.1423, -0.1435,\n","         -0.1426, -0.1424, -0.1413, -0.1416, -0.1424, -0.1424, -0.1421, -0.1415,\n","         -0.1428, -0.1420, -0.1425, -0.1415, -0.1429, -0.1424, -0.1427, -0.1420]],\n","       device='cuda:0', grad_fn=<ReshapeAliasBackward0>)\n","tensor([[0.4646, 0.4646, 0.4644, 0.4647, 0.4644, 0.4645, 0.4647, 0.4644, 0.4645,\n","         0.4642, 0.4642, 0.4646, 0.4645, 0.4647, 0.4645, 0.4642, 0.4644, 0.4645,\n","         0.4647, 0.4647, 0.4644, 0.4644, 0.4645, 0.4647, 0.4644, 0.4646, 0.4644,\n","         0.4647, 0.4643, 0.4645, 0.4644, 0.4646]], device='cuda:0',\n","       grad_fn=<ReshapeAliasBackward0>)\n","tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0',\n","       grad_fn=<ReshapeAliasBackward0>)\n","18\n","14.0\n","Train Accuracy = 47.9167\n"]},{"name":"stderr","output_type":"stream","text":["  1%|          | 4/682 [00:17<45:05,  3.99s/it]"]},{"name":"stdout","output_type":"stream","text":["tensor([[-0.1379, -0.1383, -0.1377, -0.1387, -0.1385, -0.1376, -0.1388, -0.1383,\n","         -0.1386, -0.1383, -0.1381, -0.1378, -0.1379, -0.1378, -0.1389, -0.1387,\n","         -0.1378, -0.1366, -0.1372, -0.1377, -0.1381, -0.1380, -0.1388, -0.1380,\n","         -0.1374, -0.1374, -0.1389, -0.1383, -0.1382, -0.1375, -0.1389, -0.1371]],\n","       device='cuda:0', grad_fn=<ReshapeAliasBackward0>)\n","tensor([[0.4656, 0.4655, 0.4656, 0.4654, 0.4654, 0.4657, 0.4654, 0.4655, 0.4654,\n","         0.4655, 0.4655, 0.4656, 0.4656, 0.4656, 0.4653, 0.4654, 0.4656, 0.4659,\n","         0.4657, 0.4656, 0.4655, 0.4655, 0.4654, 0.4656, 0.4657, 0.4657, 0.4653,\n","         0.4655, 0.4655, 0.4657, 0.4653, 0.4658]], device='cuda:0',\n","       grad_fn=<ReshapeAliasBackward0>)\n","tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0',\n","       grad_fn=<ReshapeAliasBackward0>)\n","20\n","12.0\n","Train Accuracy = 51.5625\n"]},{"name":"stderr","output_type":"stream","text":["  1%|          | 5/682 [00:20<42:55,  3.80s/it]"]},{"name":"stdout","output_type":"stream","text":["tensor([[-0.1400, -0.1399, -0.1404, -0.1406, -0.1406, -0.1404, -0.1401, -0.1406,\n","         -0.1404, -0.1401, -0.1404, -0.1405, -0.1396, -0.1402, -0.1397, -0.1404,\n","         -0.1401, -0.1401, -0.1402, -0.1403, -0.1403, -0.1401, -0.1412, -0.1403,\n","         -0.1405, -0.1404, -0.1400, -0.1398, -0.1403, -0.1399, -0.1401, -0.1396]],\n","       device='cuda:0', grad_fn=<ReshapeAliasBackward0>)\n","tensor([[0.4651, 0.4651, 0.4650, 0.4649, 0.4649, 0.4649, 0.4650, 0.4649, 0.4649,\n","         0.4650, 0.4650, 0.4649, 0.4651, 0.4650, 0.4651, 0.4650, 0.4650, 0.4650,\n","         0.4650, 0.4650, 0.4650, 0.4650, 0.4648, 0.4650, 0.4649, 0.4650, 0.4651,\n","         0.4651, 0.4650, 0.4651, 0.4650, 0.4652]], device='cuda:0',\n","       grad_fn=<ReshapeAliasBackward0>)\n","tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0',\n","       grad_fn=<ReshapeAliasBackward0>)\n","11\n","21.0\n","Train Accuracy = 48.1250\n"]},{"name":"stderr","output_type":"stream","text":["  1%|          | 5/682 [00:22<51:05,  4.53s/it]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_824/279048322.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m# Train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mfusion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfusion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfusion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"OVERALL - Epoch {epoch+1}/{EPOCHS}, Train Loss = {train_loss:.4f}, Train Accuracy = {train_acc:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_824/3435531890.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_to_none\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Uses less memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfusion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_824/2251000900.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, texts)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# Initialize text and visual classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mvisual_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVisual_Feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mtextual_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextual_Feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# Extract visual and textual features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_824/1251765207.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m# Define virtual bert model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mvisual_bert\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVisualBertModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'uclanlp/visualbert-vqa'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         dense_layers = nn.Sequential(\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#             nn.Linear(30522, 20000), # 30522 is the dimensions returned by the visual bert pretrained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2496\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2497\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mContextManagers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_contexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2498\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2500\u001b[0m         \u001b[0;31m# Check first if we are `from_pt`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/models/visual_bert/modeling_visual_bert.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, add_pooling_layer)\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVisualBertEmbeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 690\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVisualBertEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    691\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpooler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVisualBertPooler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0madd_pooling_layer\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/models/visual_bert/modeling_visual_bert.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModuleList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mVisualBertLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    398\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_checkpointing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/models/visual_bert/modeling_visual_bert.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModuleList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mVisualBertLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    398\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_checkpointing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/models/visual_bert/modeling_visual_bert.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVisualBertAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVisualBertIntermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 361\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVisualBertOutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m     def forward(\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/models/visual_bert/modeling_visual_bert.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm_eps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_dropout_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_parameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bias'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mreset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;31m# uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;31m# https://github.com/pytorch/pytorch/issues/57109\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkaiming_uniform_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0mfan_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calculate_fan_in_and_fan_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/init.py\u001b[0m in \u001b[0;36mkaiming_uniform_\u001b[0;34m(tensor, a, mode, nonlinearity)\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mstd\u001b[0m  \u001b[0;31m# Calculate uniform bounds from standard deviation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mbound\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["prev_dev_acc = 0\n","dev_acc = 0\n","try:\n","    for epoch in range(EPOCHS):\n","        # Train model\n","        fusion = fusion.train()\n","        train_acc, train_loss = train_model(fusion)\n","        print(f\"OVERALL - Epoch {epoch+1}/{EPOCHS}, Train Loss = {train_loss:.4f}, Train Accuracy = {train_acc:.4f}\")\n","\n","        # Evaluate model\n","        fusion.eval()\n","        dev_acc, dev_loss = eval_model(fusion)\n","        print(f\"OVERALL - Epoch {epoch+1}/{EPOCHS}, Dev Loss = {dev_loss:.4f}, Dev Accuracy = {dev_acc:.4f}\")\n","\n","        # Save best model\n","        prev_dev_acc = save_model(prev_dev_acc, dev_acc, epoch + 1, fusion, optimizer)\n","    \n","except Exception as e:\n","    # Log the exception\n","    print(e)\n","\n","    # Save best model\n","    prev_dev_acc = save_model(prev_dev_acc, dev_acc, epoch, fusion, optimizer)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"hmc","language":"python","name":"venv"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":4}
