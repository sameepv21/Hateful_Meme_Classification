{"cells":[{"cell_type":"code","execution_count":46,"metadata":{"execution":{"iopub.execute_input":"2023-04-17T06:17:52.789099Z","iopub.status.busy":"2023-04-17T06:17:52.788301Z","iopub.status.idle":"2023-04-17T06:17:55.850681Z","shell.execute_reply":"2023-04-17T06:17:55.849374Z","shell.execute_reply.started":"2023-04-17T06:17:52.789067Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import torchvision.transforms as transforms\n","from torchvision import models\n","from transformers import BertTokenizer, VisualBertModel, VisualBertForPreTraining, logging\n","from PIL import Image\n","from tqdm import tqdm\n","import os\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","logging.set_verbosity_error()\n","import numpy as np"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-04-17T06:17:55.858650Z","iopub.status.busy":"2023-04-17T06:17:55.857944Z","iopub.status.idle":"2023-04-17T06:17:55.950280Z","shell.execute_reply":"2023-04-17T06:17:55.949398Z","shell.execute_reply.started":"2023-04-17T06:17:55.858608Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>img</th>\n","      <th>text</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>42953</td>\n","      <td>train/non_hateful/42953.png</td>\n","      <td>it their charact not their color that matter</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>23058</td>\n","      <td>train/non_hateful/23058.png</td>\n","      <td>dont be afraid to love again everyon is not li...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>13894</td>\n","      <td>train/non_hateful/13894.png</td>\n","      <td>put bow on your pet</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>37408</td>\n","      <td>train/non_hateful/37408.png</td>\n","      <td>i love everyth and everybodi except for squirr...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>82403</td>\n","      <td>train/non_hateful/82403.png</td>\n","      <td>everybodi love chocol chip cooki even hitler</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      id                          img  \\\n","0  42953  train/non_hateful/42953.png   \n","1  23058  train/non_hateful/23058.png   \n","2  13894  train/non_hateful/13894.png   \n","3  37408  train/non_hateful/37408.png   \n","4  82403  train/non_hateful/82403.png   \n","\n","                                                text  label  \n","0       it their charact not their color that matter      0  \n","1  dont be afraid to love again everyon is not li...      0  \n","2                                put bow on your pet      0  \n","3  i love everyth and everybodi except for squirr...      0  \n","4       everybodi love chocol chip cooki even hitler      0  "]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["train_df = pd.read_json(\"../data/facebook/train.json\")\n","dev_df = pd.read_json(\"../data/facebook/dev.json\")\n","train_df.head()"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2023-04-17T06:17:55.956717Z","iopub.status.busy":"2023-04-17T06:17:55.954383Z","iopub.status.idle":"2023-04-17T06:17:56.062625Z","shell.execute_reply":"2023-04-17T06:17:56.061315Z","shell.execute_reply.started":"2023-04-17T06:17:55.956679Z"},"trusted":true},"outputs":[],"source":["# Some global variables\n","BATCH_SIZE = 32\n","EPOCHS = 5\n","ROOT_PATH = '../data/facebook'\n","IMAGE_SIZE = 224*224\n","NUM_CLASSES = 2\n","TEXTUAL_DIMENSION = 512\n","VISUAL_DIMENSION = 512\n","CHECKPOINT = './model.pt'\n","DEVICE = torch.device('cuda' if torch.cuda.is_available() else'cpu')"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2023-04-17T06:17:56.073391Z","iopub.status.busy":"2023-04-17T06:17:56.071005Z","iopub.status.idle":"2023-04-17T06:17:56.084176Z","shell.execute_reply":"2023-04-17T06:17:56.083093Z","shell.execute_reply.started":"2023-04-17T06:17:56.073351Z"},"trusted":true},"outputs":[],"source":["# Initialize the dataset and maintain the dataloader\n","class DynamicDataset(Dataset):\n","    def __init__(self, json_path, transform = None):\n","        self.df = pd.read_json(json_path)\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.df)\n","    \n","    def __getitem__(self, index):\n","        img_path = self.df.loc[index, 'img']\n","        img_file = os.path.join(ROOT_PATH, img_path)\n","        image = Image.open(img_file).convert(\"RGB\")\n","        if self.transform is not None:\n","            image = self.transform(image)\n","        \n","        text = self.df.loc[index, 'text']\n","        if 'label' not in self.df.columns:\n","            return image, text\n","        label = self.df.loc[index, 'label']\n","\n","        return image ,text, label"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2023-04-17T06:17:56.092175Z","iopub.status.busy":"2023-04-17T06:17:56.089202Z","iopub.status.idle":"2023-04-17T06:17:56.165952Z","shell.execute_reply":"2023-04-17T06:17:56.164772Z","shell.execute_reply.started":"2023-04-17T06:17:56.092137Z"},"trusted":true},"outputs":[],"source":["# Define a transform function for image preprocessing\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","])\n","\n","# Create objects of each set of data\n","train_data = DynamicDataset(os.path.join(ROOT_PATH, 'train.json'), transform = transform)\n","dev_data = DynamicDataset(os.path.join(ROOT_PATH, 'dev.json'), transform = transform)\n","\n","# Create a dataloader\n","train_loader = DataLoader(train_data, batch_size = BATCH_SIZE, shuffle = True)\n","dev_loader = DataLoader(dev_data, batch_size = BATCH_SIZE, shuffle = True)"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2023-04-17T06:17:56.173876Z","iopub.status.busy":"2023-04-17T06:17:56.171362Z","iopub.status.idle":"2023-04-17T06:17:56.187464Z","shell.execute_reply":"2023-04-17T06:17:56.186206Z","shell.execute_reply.started":"2023-04-17T06:17:56.173831Z"},"trusted":true},"outputs":[],"source":["class Visual_Feature(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","        # Define resnet50 model\n","        resnet50 = models.resnet50(weights = models.ResNet50_Weights.DEFAULT)\n","        convolution_layers = nn.Sequential(\n","            nn.Conv2d(2048, 1024, kernel_size=(3, 3), stride = (1, 1), padding = (1, 1)),\n","            nn.ReLU(),\n","            nn.Conv2d(1024, 512, kernel_size=(3, 3), stride = (1, 1), padding = (1, 1)),\n","            nn.ReLU(),\n","        )\n","        \n","        # Freeze parameters\n","        for param in resnet50.parameters():\n","            param.requires_grad = False\n","\n","        self.resnet50 = nn.Sequential(*list(resnet50.children())[:-1])\n","        self.convolution_layers = convolution_layers\n","\n","    def get_visual_features(self, images, get_conv_features):\n","        # Extract visual features from resnet50 model\n","        visual_features = None\n","        if(get_conv_features):\n","            visual_features = self.convolution_layers(self.resnet50(images))\n","        else:\n","            visual_features = self.resnet50(images)\n","        visual_features = visual_features.view(visual_features.size(0), -1)\n","        \n","        return visual_features"]},{"cell_type":"code","execution_count":49,"metadata":{"execution":{"iopub.execute_input":"2023-04-17T06:17:56.196304Z","iopub.status.busy":"2023-04-17T06:17:56.192913Z","iopub.status.idle":"2023-04-17T06:17:56.214149Z","shell.execute_reply":"2023-04-17T06:17:56.212954Z","shell.execute_reply.started":"2023-04-17T06:17:56.196260Z"},"trusted":true},"outputs":[],"source":["class Textual_Feature(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","        # Define virtual bert model\n","        visual_bert = VisualBertForPreTraining.from_pretrained('uclanlp/visualbert-vqa')\n","        dense_layers = nn.Sequential(\n","            nn.Linear(768, 512),\n","            nn.ReLU(),\n","        )\n","        \n","#         # Freeze parameters\n","#         for param in visual_bert.parameters():\n","#             param.requires_grad = False\n","\n","        self.visual_bert = visual_bert\n","        self.dense_layers = dense_layers\n","\n","        # Define tokenizer\n","        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","    \n","    def get_textual_features(self, images, texts):\n","        # Define indices and attention mask\n","        inputs = self.tokenizer(texts, padding = True, return_tensors = 'pt')\n","        input_ids = inputs['input_ids'].to(DEVICE)\n","        attention_mask = inputs['attention_mask'].to(DEVICE)\n","        token_ids = inputs['token_type_ids'].to(DEVICE)\n","\n","        # Extract visual features\n","        resnet50 = Visual_Feature()\n","        resnet50.to(DEVICE)\n","        visual_features = resnet50.get_visual_features(images.to(DEVICE), get_conv_features = False).unsqueeze(0)\n","        visual_token_ids = torch.ones(visual_features.shape[:-1], dtype=torch.long).to(DEVICE).reshape(-1, 1)\n","        visual_attention_mask = torch.ones(visual_features.shape[:-1], dtype=torch.float).to(DEVICE).reshape(-1, 1)\n","\n","        # input_tensor = torch.cat((visual_features, input_ids), dim=1) # concatenate visual features and token ids\n","\n","        print(visual_features.shape)\n","        print(visual_token_ids.shape)\n","        print(visual_attention_mask.shape)\n","        print(inputs['attention_mask'].shape)\n","        print(inputs['token_type_ids'].shape)\n","\n","        sentence_image_labels = torch.tensor(BATCH_SIZE).unsqueeze(0)  # Batch_size\n","\n","        inputs.update(\n","            {\n","                \"visual_embeds\": visual_features,\n","                \"visual_token_type_ids\": visual_token_ids,\n","                \"visual_attention_mask\": visual_attention_mask,\n","                \"sentence_image_labels\": sentence_image_labels,\n","            }\n","        )\n","\n","        # Extract textual features from virtual bert model\n","        textual_features = self.visual_bert(\n","            **inputs\n","        )\n","        \n","        textual_features = textual_features[0][:, 0, :] # Extract the first token of last hidden state\n","        textual_features = self.dense_layers(textual_features)\n","\n","        return textual_features"]},{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/341 [00:00<?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["torch.Size([32, 3, 224, 224])\n","(32,)\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/341 [00:04<?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["torch.Size([1, 32, 2048])\n","torch.Size([32, 1])\n","torch.Size([32, 1])\n","torch.Size([32, 35])\n","torch.Size([32, 35])\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"ename":"RuntimeError","evalue":"The size of tensor a (67) must match the size of tensor b (36) at non-singleton dimension 3","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[50], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mprint\u001b[39m(np\u001b[39m.\u001b[39marray(texts)\u001b[39m.\u001b[39mshape)\n\u001b[1;32m      6\u001b[0m labels \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mreshape(labels, (\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m))\u001b[39m.\u001b[39mto(dtype \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfloat32, device \u001b[39m=\u001b[39m DEVICE)\n\u001b[0;32m----> 8\u001b[0m textual_feature \u001b[39m=\u001b[39m vbert\u001b[39m.\u001b[39;49mget_textual_features(images, texts)\n\u001b[1;32m      9\u001b[0m \u001b[39mprint\u001b[39m(textual_feature\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     10\u001b[0m \u001b[39mbreak\u001b[39;00m\n","Cell \u001b[0;32mIn[49], line 56\u001b[0m, in \u001b[0;36mTextual_Feature.get_textual_features\u001b[0;34m(self, images, texts)\u001b[0m\n\u001b[1;32m     46\u001b[0m inputs\u001b[39m.\u001b[39mupdate(\n\u001b[1;32m     47\u001b[0m     {\n\u001b[1;32m     48\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mvisual_embeds\u001b[39m\u001b[39m\"\u001b[39m: visual_features,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     52\u001b[0m     }\n\u001b[1;32m     53\u001b[0m )\n\u001b[1;32m     55\u001b[0m \u001b[39m# Extract textual features from virtual bert model\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m textual_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvisual_bert(\n\u001b[1;32m     57\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs\n\u001b[1;32m     58\u001b[0m )\n\u001b[1;32m     60\u001b[0m textual_features \u001b[39m=\u001b[39m textual_features[\u001b[39m0\u001b[39m][:, \u001b[39m0\u001b[39m, :] \u001b[39m# Extract the first token of last hidden state\u001b[39;00m\n\u001b[1;32m     61\u001b[0m textual_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdense_layers(textual_features)\n","File \u001b[0;32m~/Extra_Projects/Hateful_Meme_Classification/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/Extra_Projects/Hateful_Meme_Classification/venv/lib/python3.8/site-packages/transformers/models/visual_bert/modeling_visual_bert.py:960\u001b[0m, in \u001b[0;36mVisualBertForPreTraining.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, visual_embeds, visual_attention_mask, visual_token_type_ids, image_text_alignment, output_attentions, output_hidden_states, return_dict, labels, sentence_image_labels)\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    912\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size, total_sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \u001b[39m    Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    956\u001b[0m \u001b[39mseq_relationship_logits = outputs.seq_relationship_logits\u001b[39;00m\n\u001b[1;32m    957\u001b[0m \u001b[39m```\"\"\"\u001b[39;00m\n\u001b[1;32m    958\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m--> 960\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvisual_bert(\n\u001b[1;32m    961\u001b[0m     input_ids,\n\u001b[1;32m    962\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    963\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m    964\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    965\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    966\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    967\u001b[0m     visual_embeds\u001b[39m=\u001b[39;49mvisual_embeds,\n\u001b[1;32m    968\u001b[0m     visual_attention_mask\u001b[39m=\u001b[39;49mvisual_attention_mask,\n\u001b[1;32m    969\u001b[0m     visual_token_type_ids\u001b[39m=\u001b[39;49mvisual_token_type_ids,\n\u001b[1;32m    970\u001b[0m     image_text_alignment\u001b[39m=\u001b[39;49mimage_text_alignment,\n\u001b[1;32m    971\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    972\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    973\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    974\u001b[0m )\n\u001b[1;32m    976\u001b[0m sequence_output, pooled_output \u001b[39m=\u001b[39m outputs[:\u001b[39m2\u001b[39m]\n\u001b[1;32m    977\u001b[0m prediction_scores, seq_relationship_score \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcls(sequence_output, pooled_output)\n","File \u001b[0;32m~/Extra_Projects/Hateful_Meme_Classification/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/Extra_Projects/Hateful_Meme_Classification/venv/lib/python3.8/site-packages/transformers/models/visual_bert/modeling_visual_bert.py:843\u001b[0m, in \u001b[0;36mVisualBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, visual_embeds, visual_attention_mask, visual_token_type_ids, image_text_alignment, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    840\u001b[0m     pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    842\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 843\u001b[0m     encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m    844\u001b[0m         embedding_output,\n\u001b[1;32m    845\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m    846\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    847\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    848\u001b[0m         output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    849\u001b[0m         return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    850\u001b[0m     )\n\u001b[1;32m    851\u001b[0m     sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    853\u001b[0m     pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n","File \u001b[0;32m~/Extra_Projects/Hateful_Meme_Classification/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/Extra_Projects/Hateful_Meme_Classification/venv/lib/python3.8/site-packages/transformers/models/visual_bert/modeling_visual_bert.py:433\u001b[0m, in \u001b[0;36mVisualBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    426\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    427\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    428\u001b[0m         hidden_states,\n\u001b[1;32m    429\u001b[0m         attention_mask,\n\u001b[1;32m    430\u001b[0m         layer_head_mask,\n\u001b[1;32m    431\u001b[0m     )\n\u001b[1;32m    432\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 433\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(hidden_states, attention_mask, layer_head_mask, output_attentions)\n\u001b[1;32m    435\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    436\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n","File \u001b[0;32m~/Extra_Projects/Hateful_Meme_Classification/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/Extra_Projects/Hateful_Meme_Classification/venv/lib/python3.8/site-packages/transformers/models/visual_bert/modeling_visual_bert.py:370\u001b[0m, in \u001b[0;36mVisualBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    364\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    365\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    368\u001b[0m     output_attentions\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    369\u001b[0m ):\n\u001b[0;32m--> 370\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    371\u001b[0m         hidden_states,\n\u001b[1;32m    372\u001b[0m         attention_mask,\n\u001b[1;32m    373\u001b[0m         head_mask,\n\u001b[1;32m    374\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    375\u001b[0m     )\n\u001b[1;32m    376\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    378\u001b[0m     outputs \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add self attentions if we output attention weights\u001b[39;00m\n","File \u001b[0;32m~/Extra_Projects/Hateful_Meme_Classification/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/Extra_Projects/Hateful_Meme_Classification/venv/lib/python3.8/site-packages/transformers/models/visual_bert/modeling_visual_bert.py:312\u001b[0m, in \u001b[0;36mVisualBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    306\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    307\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    310\u001b[0m     output_attentions\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    311\u001b[0m ):\n\u001b[0;32m--> 312\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[1;32m    313\u001b[0m         hidden_states,\n\u001b[1;32m    314\u001b[0m         attention_mask,\n\u001b[1;32m    315\u001b[0m         head_mask,\n\u001b[1;32m    316\u001b[0m         output_attentions,\n\u001b[1;32m    317\u001b[0m     )\n\u001b[1;32m    318\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[1;32m    319\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n","File \u001b[0;32m~/Extra_Projects/Hateful_Meme_Classification/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/Extra_Projects/Hateful_Meme_Classification/venv/lib/python3.8/site-packages/transformers/models/visual_bert/modeling_visual_bert.py:241\u001b[0m, in \u001b[0;36mVisualBertSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    238\u001b[0m attention_scores \u001b[39m=\u001b[39m attention_scores \u001b[39m/\u001b[39m math\u001b[39m.\u001b[39msqrt(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention_head_size)\n\u001b[1;32m    239\u001b[0m \u001b[39mif\u001b[39;00m attention_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    240\u001b[0m     \u001b[39m# Apply the attention mask is (precomputed for all layers in VisualBertSelfAttentionModel forward() function)\u001b[39;00m\n\u001b[0;32m--> 241\u001b[0m     attention_scores \u001b[39m=\u001b[39m attention_scores \u001b[39m+\u001b[39;49m attention_mask\n\u001b[1;32m    243\u001b[0m \u001b[39m# Normalize the attention scores to probabilities.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m attention_probs \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39msoftmax(attention_scores, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n","\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (67) must match the size of tensor b (36) at non-singleton dimension 3"]}],"source":["vbert = Textual_Feature()\n","for images, texts, labels in tqdm(train_loader):\n","    images = images.to(DEVICE)\n","    print(images.shape)\n","    print(np.array(texts).shape)\n","    labels = torch.reshape(labels, (-1, 1)).to(dtype = torch.float32, device = DEVICE)\n","\n","    textual_feature = vbert.get_textual_features(images, texts)\n","    print(textual_feature.shape)\n","    break"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\"\n","# Assumption: *get_visual_embeddings(image)* gets the visual embeddings of the image in the batch.\n","from transformers import AutoTokenizer, VisualBertForPreTraining\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n","model = VisualBertForPreTraining.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")\n","\n","inputs = tokenizer(\"The capital of France is [MASK].\", return_tensors=\"pt\")\n","visual_embeds = get_visual_embeddings(image).unsqueeze(0)\n","visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.long)\n","visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.float)\n","\n","inputs.update(\n","    {\n","        \"visual_embeds\": visual_embeds,\n","        \"visual_token_type_ids\": visual_token_type_ids,\n","        \"visual_attention_mask\": visual_attention_mask,\n","    }\n",")\n","max_length = inputs[\"input_ids\"].shape[-1] + visual_embeds.shape[-2]\n","labels = tokenizer(\n","    \"The capital of France is Paris.\", return_tensors=\"pt\", padding=\"max_length\", max_length=max_length\n",")[\"input_ids\"]\n","sentence_image_labels = torch.tensor(1).unsqueeze(0)  # Batch_size\n","\n","\n","outputs = model(**inputs, labels=labels, sentence_image_labels=sentence_image_labels)\n","loss = outputs.loss\n","prediction_logits = outputs.prediction_logits\n","seq_relationship_logits = outputs.seq_relationship_logits\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-17T06:17:56.222641Z","iopub.status.busy":"2023-04-17T06:17:56.219823Z","iopub.status.idle":"2023-04-17T06:18:02.852912Z","shell.execute_reply":"2023-04-17T06:18:02.851692Z","shell.execute_reply.started":"2023-04-17T06:17:56.222600Z"},"trusted":true},"outputs":[{"ename":"NameError","evalue":"name 'Textual_Feature' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Test visual bert (WORKS)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m vbert \u001b[39m=\u001b[39m Textual_Feature()\n\u001b[1;32m      3\u001b[0m vbert\u001b[39m.\u001b[39mto(DEVICE)\n\u001b[1;32m      4\u001b[0m text \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mThis is a test sentence\u001b[39m\u001b[39m\"\u001b[39m]\n","\u001b[0;31mNameError\u001b[0m: name 'Textual_Feature' is not defined"]}],"source":["# Test visual bert (WORKS)\n","vbert = Textual_Feature()\n","vbert.to(DEVICE)\n","text = [\"This is a test sentence\"]\n","image = Image.open(os.path.join(ROOT_PATH, 'dev/hateful/01456.png'))\n","image = transform(image).reshape(1, 3, 224, 224)\n","textual_features = vbert.get_textual_features(image, text)\n","print(textual_features.shape) # Shape of last hidden state"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-17T06:18:02.855636Z","iopub.status.busy":"2023-04-17T06:18:02.854599Z","iopub.status.idle":"2023-04-17T06:18:03.711728Z","shell.execute_reply":"2023-04-17T06:18:03.710489Z","shell.execute_reply.started":"2023-04-17T06:18:02.855598Z"},"trusted":true},"outputs":[],"source":["# Test resnet50 (WORKS)\n","resnet50 = Visual_Feature()\n","resnet50.to(DEVICE)\n","image = Image.open(os.path.join(ROOT_PATH, 'dev/hateful/01456.png'))\n","image = transform(image).reshape(1, 3, 224, 224)\n","visual_features = resnet50.get_visual_features(image.to(DEVICE), get_conv_features = True)\n","print(visual_features.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-17T06:18:03.715233Z","iopub.status.busy":"2023-04-17T06:18:03.714123Z","iopub.status.idle":"2023-04-17T06:18:03.724880Z","shell.execute_reply":"2023-04-17T06:18:03.723704Z","shell.execute_reply.started":"2023-04-17T06:18:03.715186Z"},"trusted":true},"outputs":[],"source":["class Fusion(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","        # Define fusion layers\n","        fusion_layers = nn.Sequential(\n","            nn.Linear((VISUAL_DIMENSION + TEXTUAL_DIMENSION), 512),\n","            nn.ReLU(),\n","            nn.Linear(512, 256),\n","            nn.ReLU(),\n","            nn.Linear(256, 128),\n","            nn.ReLU(),\n","            nn.Linear(128, 64),\n","            nn.ReLU(),\n","            nn.Linear(64, 32),\n","            nn.ReLU(),\n","            nn.Linear(32, 1),\n","        )\n","\n","        self.fusion_layers = fusion_layers\n","    \n","    def forward(self, images, texts):\n","        # Initialize text and visual classes\n","        visual_class = Visual_Feature().to(DEVICE)\n","        textual_class = Textual_Feature().to(DEVICE)\n","\n","        # Extract visual and textual features\n","        visual_features = visual_class.get_visual_features(images, get_conv_features = True)\n","\n","        textual_features = textual_class.get_textual_features(images, texts)\n","        print(\"\\n\\nHERE 1\\n\\n\")\n","\n","        # Concatenate visual and textual features\n","        features = torch.cat((visual_features, textual_features), dim = 1)\n","        print(\"\\n\\nHERE 2\\n\\n\")\n","\n","        # Pass through fusion layers\n","        output = self.fusion_layers(features)\n","        print(\"\\n\\nHERE 3\\n\\n\")\n","\n","        return output"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-17T06:18:03.728831Z","iopub.status.busy":"2023-04-17T06:18:03.728049Z","iopub.status.idle":"2023-04-17T06:18:03.745827Z","shell.execute_reply":"2023-04-17T06:18:03.744630Z","shell.execute_reply.started":"2023-04-17T06:18:03.728790Z"},"trusted":true},"outputs":[],"source":["# Define fusion model\n","fusion = Fusion()\n","fusion.to(DEVICE)\n","\n","# Define loss function and optimizer\n","criterion = nn.BCEWithLogitsLoss()\n","\n","# Define optimizer\n","optimizer = optim.Adam(fusion.parameters(), lr = 0.01)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-17T06:18:03.749170Z","iopub.status.busy":"2023-04-17T06:18:03.748893Z","iopub.status.idle":"2023-04-17T06:18:03.757475Z","shell.execute_reply":"2023-04-17T06:18:03.756327Z","shell.execute_reply.started":"2023-04-17T06:18:03.749143Z"},"trusted":true},"outputs":[],"source":["def train_model(model):\n","    # Initialize required variables\n","    train_loss = 0\n","    train_acc = 0\n","    total = 0\n","    correct = 0\n","    \n","    for images, texts, labels in tqdm(train_loader):\n","        images = images.to(DEVICE)\n","        labels = torch.reshape(labels, (-1, 1)).to(dtype = torch.float32, device = DEVICE)\n","\n","        optimizer.zero_grad()\n","        outputs = fusion(images, texts)\n","        # print(\"\\n\\nHERE 1\\n\\n\")\n","\n","        predicted = torch.round(torch.sigmoid(outputs))\n","        # print(\"\\n\\nHERE 2\\n\\n\")\n","\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","        # print(\"\\n\\nHERE 3\\n\\n\")\n","\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        train_loss += loss.item() * images.size(0)\n","    train_acc = 100 * correct / total\n","    train_loss /= len(train_data)\n","    return train_acc, train_loss\n","        "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-17T06:18:03.762668Z","iopub.status.busy":"2023-04-17T06:18:03.761796Z","iopub.status.idle":"2023-04-17T06:18:03.773018Z","shell.execute_reply":"2023-04-17T06:18:03.771964Z","shell.execute_reply.started":"2023-04-17T06:18:03.762638Z"},"trusted":true},"outputs":[],"source":["def eval_model(model):    \n","    # Initialize the required variables\n","    dev_loss = 0\n","    dev_acc = 0\n","    total = 0\n","    correct = 0\n","    \n","    for images, texts, labels in tqdm(dev_loader):\n","        images = images.to(DEVICE)\n","        labels = torch.reshape(labels, (-1, 1)).to(dtype = torch.float32, device = DEVICE)\n","        \n","        outputs = model(images, texts)\n","        predicted = torch.round(torch.sigmoid(outputs)) # threshold issues\n","        \n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","        \n","        loss = criterion(outputs, labels)\n","        dev_loss += loss.item() * images.size(0)\n","        \n","    dev_acc = 100 * correct / total\n","    dev_loss /= len(dev_data)\n","    \n","    return dev_acc, dev_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-17T06:18:03.777119Z","iopub.status.busy":"2023-04-17T06:18:03.776742Z","iopub.status.idle":"2023-04-17T06:18:03.785177Z","shell.execute_reply":"2023-04-17T06:18:03.783831Z","shell.execute_reply.started":"2023-04-17T06:18:03.777090Z"},"trusted":true},"outputs":[],"source":["def save_model(prev_acc, curr_acc, epoch, model, optimizer):\n","    # Compare and save\n","    if curr_acc > prev_acc:\n","        # Save the model\n","        torch.save({\n","            'epoch': epoch,\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","        }, CHECKPOINT)\n","        \n","        # Return new highest accuracy\n","        return curr_acc\n","    return prev_acc"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-17T06:18:03.787691Z","iopub.status.busy":"2023-04-17T06:18:03.787094Z","iopub.status.idle":"2023-04-17T06:18:09.595543Z","shell.execute_reply":"2023-04-17T06:18:09.594311Z","shell.execute_reply.started":"2023-04-17T06:18:03.787651Z"},"trusted":true},"outputs":[],"source":["prev_dev_acc = 0\n","dev_acc = 0\n","# try:\n","for epoch in range(EPOCHS):\n","    # Train model\n","    fusion.train()\n","    train_acc, train_loss = train_model(fusion)\n","    print(f\"Epoch {epoch+1}/{EPOCHS}, Train Loss = {train_loss:.4f}, Train Accuracy = {train_acc:.4f}\")\n","\n","    # Evaluate model\n","    fusion.eval()\n","    dev_acc, dev_loss = eval_model(fusion)\n","    print(f\"Epoch {epoch+1}/{EPOCHS}, Dev Loss = {dev_loss:.4f}, Dev Accuracy = {dev_acc:.4f}\")\n","\n","    # Save best model\n","    prev_dev_acc = save_model(prev_dev_acc, dev_acc, epoch + 1, fusion, optimizer)\n","        \n","# except Exception as e:\n","#     # Log the exception\n","#     print(e)\n","\n","#     # Save best model\n","#     prev_dev_acc = save_model(prev_dev_acc, dev_acc, epoch, fusion, optimizer)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"hmc","language":"python","name":"venv"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":4}
