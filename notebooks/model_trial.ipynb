{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-04-17T06:17:52.789099Z","iopub.status.busy":"2023-04-17T06:17:52.788301Z","iopub.status.idle":"2023-04-17T06:17:55.850681Z","shell.execute_reply":"2023-04-17T06:17:55.849374Z","shell.execute_reply.started":"2023-04-17T06:17:52.789067Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/sameep/Extra_Projects/Hateful_Meme_Classification/venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import torchvision.transforms as transforms\n","from torchvision import models\n","from transformers import BertTokenizer, VisualBertModel, VisualBertForPreTraining, logging\n","from PIL import Image\n","from tqdm import tqdm\n","import os\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","logging.set_verbosity_error()\n","import numpy as np"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-04-17T06:17:55.858650Z","iopub.status.busy":"2023-04-17T06:17:55.857944Z","iopub.status.idle":"2023-04-17T06:17:55.950280Z","shell.execute_reply":"2023-04-17T06:17:55.949398Z","shell.execute_reply.started":"2023-04-17T06:17:55.858608Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>img</th>\n","      <th>text</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>42953</td>\n","      <td>train/non_hateful/42953.png</td>\n","      <td>it their charact not their color that matter</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>23058</td>\n","      <td>train/non_hateful/23058.png</td>\n","      <td>dont be afraid to love again everyon is not li...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>13894</td>\n","      <td>train/non_hateful/13894.png</td>\n","      <td>put bow on your pet</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>37408</td>\n","      <td>train/non_hateful/37408.png</td>\n","      <td>i love everyth and everybodi except for squirr...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>82403</td>\n","      <td>train/non_hateful/82403.png</td>\n","      <td>everybodi love chocol chip cooki even hitler</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      id                          img  \\\n","0  42953  train/non_hateful/42953.png   \n","1  23058  train/non_hateful/23058.png   \n","2  13894  train/non_hateful/13894.png   \n","3  37408  train/non_hateful/37408.png   \n","4  82403  train/non_hateful/82403.png   \n","\n","                                                text  label  \n","0       it their charact not their color that matter      0  \n","1  dont be afraid to love again everyon is not li...      0  \n","2                                put bow on your pet      0  \n","3  i love everyth and everybodi except for squirr...      0  \n","4       everybodi love chocol chip cooki even hitler      0  "]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["train_df = pd.read_json(\"../data/facebook/train.json\")\n","dev_df = pd.read_json(\"../data/facebook/dev.json\")\n","train_df.head()"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-04-17T06:17:55.956717Z","iopub.status.busy":"2023-04-17T06:17:55.954383Z","iopub.status.idle":"2023-04-17T06:17:56.062625Z","shell.execute_reply":"2023-04-17T06:17:56.061315Z","shell.execute_reply.started":"2023-04-17T06:17:55.956679Z"},"trusted":true},"outputs":[],"source":["# Some global variables\n","BATCH_SIZE = 32\n","EPOCHS = 5\n","ROOT_PATH = '../data/facebook'\n","IMAGE_SIZE = 224*224\n","NUM_CLASSES = 2\n","TEXTUAL_DIMENSION = 512\n","VISUAL_DIMENSION = 512\n","CHECKPOINT = './model.pt'\n","DEVICE = torch.device('cuda' if torch.cuda.is_available() else'cpu')"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-04-17T06:17:56.073391Z","iopub.status.busy":"2023-04-17T06:17:56.071005Z","iopub.status.idle":"2023-04-17T06:17:56.084176Z","shell.execute_reply":"2023-04-17T06:17:56.083093Z","shell.execute_reply.started":"2023-04-17T06:17:56.073351Z"},"trusted":true},"outputs":[],"source":["# Initialize the dataset and maintain the dataloader\n","class DynamicDataset(Dataset):\n","    def __init__(self, json_path, transform = None):\n","        self.df = pd.read_json(json_path)\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.df)\n","    \n","    def __getitem__(self, index):\n","        img_path = self.df.loc[index, 'img']\n","        img_file = os.path.join(ROOT_PATH, img_path)\n","        image = Image.open(img_file).convert(\"RGB\")\n","        if self.transform is not None:\n","            image = self.transform(image)\n","        \n","        text = self.df.loc[index, 'text']\n","        if 'label' not in self.df.columns:\n","            return image, text\n","        label = self.df.loc[index, 'label']\n","\n","        return image ,text, label"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-04-17T06:17:56.092175Z","iopub.status.busy":"2023-04-17T06:17:56.089202Z","iopub.status.idle":"2023-04-17T06:17:56.165952Z","shell.execute_reply":"2023-04-17T06:17:56.164772Z","shell.execute_reply.started":"2023-04-17T06:17:56.092137Z"},"trusted":true},"outputs":[],"source":["# Define a transform function for image preprocessing\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","])\n","\n","# Create objects of each set of data\n","train_data = DynamicDataset(os.path.join(ROOT_PATH, 'train.json'), transform = transform)\n","dev_data = DynamicDataset(os.path.join(ROOT_PATH, 'dev.json'), transform = transform)\n","\n","# Create a dataloader\n","train_loader = DataLoader(train_data, batch_size = BATCH_SIZE, shuffle = True)\n","dev_loader = DataLoader(dev_data, batch_size = BATCH_SIZE, shuffle = True)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-04-17T06:17:56.173876Z","iopub.status.busy":"2023-04-17T06:17:56.171362Z","iopub.status.idle":"2023-04-17T06:17:56.187464Z","shell.execute_reply":"2023-04-17T06:17:56.186206Z","shell.execute_reply.started":"2023-04-17T06:17:56.173831Z"},"trusted":true},"outputs":[],"source":["class Visual_Feature(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","        # Define resnet50 model\n","        resnet50 = models.resnet50(weights = models.ResNet50_Weights.DEFAULT)\n","        convolution_layers = nn.Sequential(\n","            nn.Conv2d(2048, 1024, kernel_size=(3, 3), stride = (1, 1), padding = (1, 1)),\n","            nn.ReLU(),\n","            nn.Conv2d(1024, 512, kernel_size=(3, 3), stride = (1, 1), padding = (1, 1)),\n","            nn.ReLU(),\n","        )\n","        \n","        # Freeze parameters\n","        for param in resnet50.parameters():\n","            param.requires_grad = False\n","\n","        self.resnet50 = nn.Sequential(*list(resnet50.children())[:-1])\n","        self.convolution_layers = convolution_layers\n","\n","    def get_visual_features(self, images, get_conv_features):\n","        # Extract visual features from resnet50 model\n","        visual_features = None\n","        if(get_conv_features):\n","            visual_features = self.convolution_layers(self.resnet50(images))\n","        else:\n","            visual_features = self.resnet50(images)\n","\n","        # visual_features = visual_features.view(visual_features.size(0), -1)\n","        visual_features = visual_features.reshape(BATCH_SIZE, 1, -1)\n","\n","        return visual_features"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-04-17T06:17:56.196304Z","iopub.status.busy":"2023-04-17T06:17:56.192913Z","iopub.status.idle":"2023-04-17T06:17:56.214149Z","shell.execute_reply":"2023-04-17T06:17:56.212954Z","shell.execute_reply.started":"2023-04-17T06:17:56.196260Z"},"trusted":true},"outputs":[],"source":["class Textual_Feature(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","        # Define virtual bert model\n","        visual_bert = VisualBertForPreTraining.from_pretrained('uclanlp/visualbert-vqa')\n","        dense_layers = nn.Sequential(\n","            nn.Linear(30522, 512),\n","            nn.ReLU(),\n","        )\n","        \n","#         # Freeze parameters\n","#         for param in visual_bert.parameters():\n","#             param.requires_grad = False\n","\n","        self.visual_bert = visual_bert\n","        self.dense_layers = dense_layers\n","\n","        # Define tokenizer\n","        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","    \n","    def get_textual_features(self, images, texts):\n","        # Define indices and attention mask\n","        inputs = self.tokenizer(texts, padding = True, truncation = True, return_tensors = 'pt')\n","        input_ids = inputs['input_ids'].to(DEVICE)\n","        attention_mask = inputs['attention_mask'].to(DEVICE)\n","        token_ids = inputs['token_type_ids'].to(DEVICE)\n","\n","        # Extract visual features\n","        resnet50 = Visual_Feature()\n","        resnet50.to(DEVICE)\n","        visual_features = resnet50.get_visual_features(images.to(DEVICE), get_conv_features = False)\n","        visual_token_ids = torch.ones(visual_features.shape[:-1], dtype=torch.long).to(DEVICE)\n","        visual_attention_mask = torch.ones(visual_features.shape[:-1], dtype=torch.float).to(DEVICE)\n","\n","        # print(\"Input Ids\", input_ids.shape)\n","        # print(\"Attention Mask\", attention_mask.shape)\n","        # print(\"Token Ids\", token_ids.shape)\n","        # print(\"Visual Features\", visual_features.shape)\n","        # print(\"Visual token ids\", visual_token_ids.shape)\n","        # print(\"Visual attention mask\", visual_attention_mask.shape)\n","\n","        inputs.update(\n","            {\n","                \"input_ids\": input_ids,\n","                \"attention_mask\": attention_mask,\n","                \"token_type_ids\": token_ids,\n","                \"visual_embeds\": visual_features,\n","                \"visual_token_type_ids\": visual_token_ids,\n","                \"visual_attention_mask\": visual_attention_mask,\n","            }\n","        )\n","\n","        # Extract textual features from virtual bert model\n","        textual_features = self.visual_bert(\n","            **inputs\n","        )\n","        \n","        textual_features = textual_features[0][:, 0, :] # Extract the first token of last hidden state\n","        textual_features = self.dense_layers(textual_features)\n","\n","        return textual_features"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-04-17T06:17:56.222641Z","iopub.status.busy":"2023-04-17T06:17:56.219823Z","iopub.status.idle":"2023-04-17T06:18:02.852912Z","shell.execute_reply":"2023-04-17T06:18:02.851692Z","shell.execute_reply.started":"2023-04-17T06:17:56.222600Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/341 [00:07<?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["torch.Size([32, 512])\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# Test visual bert (WORKS)\n","vbert = Textual_Feature()\n","for images, texts, labels in tqdm(train_loader):\n","    images = images.to(DEVICE)\n","    textual_feature = vbert.get_textual_features(images, texts)\n","    print(textual_feature.shape)\n","    break"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-04-17T06:18:02.855636Z","iopub.status.busy":"2023-04-17T06:18:02.854599Z","iopub.status.idle":"2023-04-17T06:18:03.711728Z","shell.execute_reply":"2023-04-17T06:18:03.710489Z","shell.execute_reply.started":"2023-04-17T06:18:02.855598Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([32, 1, 16])\n"]}],"source":["# Test resnet50 (WORKS)\n","resnet50 = Visual_Feature()\n","resnet50.to(DEVICE)\n","image = Image.open(os.path.join(ROOT_PATH, 'dev/hateful/01456.png'))\n","image = transform(image).reshape(1, 3, 224, 224)\n","visual_features = resnet50.get_visual_features(image.to(DEVICE), get_conv_features = True)\n","print(visual_features.shape)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-04-17T06:18:03.715233Z","iopub.status.busy":"2023-04-17T06:18:03.714123Z","iopub.status.idle":"2023-04-17T06:18:03.724880Z","shell.execute_reply":"2023-04-17T06:18:03.723704Z","shell.execute_reply.started":"2023-04-17T06:18:03.715186Z"},"trusted":true},"outputs":[],"source":["class Fusion(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","        # Define fusion layers\n","        fusion_layers = nn.Sequential(\n","            nn.Linear((VISUAL_DIMENSION + TEXTUAL_DIMENSION), 512),\n","            nn.ReLU(),\n","            nn.Linear(512, 256),\n","            nn.ReLU(),\n","            nn.Linear(256, 128),\n","            nn.ReLU(),\n","            nn.Linear(128, 64),\n","            nn.ReLU(),\n","            nn.Linear(64, 32),\n","            nn.ReLU(),\n","            nn.Linear(32, 1),\n","        )\n","\n","        self.fusion_layers = fusion_layers\n","    \n","    def forward(self, images, texts):\n","        # Initialize text and visual classes\n","        visual_class = Visual_Feature().to(DEVICE)\n","        textual_class = Textual_Feature().to(DEVICE)\n","\n","        # Extract visual and textual features\n","        visual_features = visual_class.get_visual_features(images, get_conv_features = True)\n","\n","        textual_features = textual_class.get_textual_features(images, texts)\n","\n","        # Concatenate visual and textual features\n","        features = torch.cat((visual_features, textual_features), dim = 1)\n","\n","        # Pass through fusion layers\n","        output = self.fusion_layers(features)\n","\n","        return output"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-04-17T06:18:03.728831Z","iopub.status.busy":"2023-04-17T06:18:03.728049Z","iopub.status.idle":"2023-04-17T06:18:03.745827Z","shell.execute_reply":"2023-04-17T06:18:03.744630Z","shell.execute_reply.started":"2023-04-17T06:18:03.728790Z"},"trusted":true},"outputs":[],"source":["# Define fusion model\n","fusion = Fusion()\n","fusion.to(DEVICE)\n","\n","# Define loss function and optimizer\n","criterion = nn.BCEWithLogitsLoss()\n","\n","# Define optimizer\n","optimizer = optim.Adam(fusion.parameters(), lr = 0.01)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-04-17T06:18:03.749170Z","iopub.status.busy":"2023-04-17T06:18:03.748893Z","iopub.status.idle":"2023-04-17T06:18:03.757475Z","shell.execute_reply":"2023-04-17T06:18:03.756327Z","shell.execute_reply.started":"2023-04-17T06:18:03.749143Z"},"trusted":true},"outputs":[],"source":["def train_model(model):\n","    # Initialize required variables\n","    train_loss = 0\n","    train_acc = 0\n","    total = 0\n","    correct = 0\n","    \n","    for images, texts, labels in tqdm(train_loader):\n","        images = images.to(DEVICE)\n","        labels = torch.reshape(labels, (-1, 1)).to(dtype = torch.float32, device = DEVICE)\n","\n","        optimizer.zero_grad()\n","        outputs = fusion(images, texts)\n","\n","        predicted = torch.round(torch.sigmoid(outputs))\n","\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        train_loss += loss.item() * images.size(0)\n","    train_acc = 100 * correct / total\n","    train_loss /= len(train_data)\n","    return train_acc, train_loss\n","        "]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-04-17T06:18:03.762668Z","iopub.status.busy":"2023-04-17T06:18:03.761796Z","iopub.status.idle":"2023-04-17T06:18:03.773018Z","shell.execute_reply":"2023-04-17T06:18:03.771964Z","shell.execute_reply.started":"2023-04-17T06:18:03.762638Z"},"trusted":true},"outputs":[],"source":["def eval_model(model):    \n","    # Initialize the required variables\n","    dev_loss = 0\n","    dev_acc = 0\n","    total = 0\n","    correct = 0\n","    \n","    for images, texts, labels in tqdm(dev_loader):\n","        images = images.to(DEVICE)\n","        labels = torch.reshape(labels, (-1, 1)).to(dtype = torch.float32, device = DEVICE)\n","        \n","        outputs = model(images, texts)\n","        predicted = torch.round(torch.sigmoid(outputs)) # threshold issues\n","        \n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","        \n","        loss = criterion(outputs, labels)\n","        dev_loss += loss.item() * images.size(0)\n","        \n","    dev_acc = 100 * correct / total\n","    dev_loss /= len(dev_data)\n","    \n","    return dev_acc, dev_loss"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-04-17T06:18:03.777119Z","iopub.status.busy":"2023-04-17T06:18:03.776742Z","iopub.status.idle":"2023-04-17T06:18:03.785177Z","shell.execute_reply":"2023-04-17T06:18:03.783831Z","shell.execute_reply.started":"2023-04-17T06:18:03.777090Z"},"trusted":true},"outputs":[],"source":["def save_model(prev_acc, curr_acc, epoch, model, optimizer):\n","    # Compare and save\n","    if curr_acc > prev_acc:\n","        # Save the model\n","        torch.save({\n","            'epoch': epoch,\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","        }, CHECKPOINT)\n","        \n","        # Return new highest accuracy\n","        return curr_acc\n","    return prev_acc"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-04-17T06:18:03.787691Z","iopub.status.busy":"2023-04-17T06:18:03.787094Z","iopub.status.idle":"2023-04-17T06:18:09.595543Z","shell.execute_reply":"2023-04-17T06:18:09.594311Z","shell.execute_reply.started":"2023-04-17T06:18:03.787651Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/341 [00:18<?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","HERE 1\n","\n","\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"ename":"RuntimeError","evalue":"Tensors must have same number of dimensions: got 3 and 2","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[15], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(EPOCHS):\n\u001b[1;32m      5\u001b[0m     \u001b[39m# Train model\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     fusion\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m----> 7\u001b[0m     train_acc, train_loss \u001b[39m=\u001b[39m train_model(fusion)\n\u001b[1;32m      8\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mEPOCHS\u001b[39m}\u001b[39;00m\u001b[39m, Train Loss = \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Train Accuracy = \u001b[39m\u001b[39m{\u001b[39;00mtrain_acc\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m     \u001b[39m# Evaluate model\u001b[39;00m\n","Cell \u001b[0;32mIn[12], line 13\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     10\u001b[0m labels \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mreshape(labels, (\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m))\u001b[39m.\u001b[39mto(dtype \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfloat32, device \u001b[39m=\u001b[39m DEVICE)\n\u001b[1;32m     12\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 13\u001b[0m outputs \u001b[39m=\u001b[39m fusion(images, texts)\n\u001b[1;32m     14\u001b[0m \u001b[39m# print(\"\\n\\nHERE 1\\n\\n\")\u001b[39;00m\n\u001b[1;32m     16\u001b[0m predicted \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mround(torch\u001b[39m.\u001b[39msigmoid(outputs))\n","File \u001b[0;32m~/Extra_Projects/Hateful_Meme_Classification/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","Cell \u001b[0;32mIn[10], line 34\u001b[0m, in \u001b[0;36mFusion.forward\u001b[0;34m(self, images, texts)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mHERE 1\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     33\u001b[0m \u001b[39m# Concatenate visual and textual features\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m features \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcat((visual_features, textual_features), dim \u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m)\n\u001b[1;32m     35\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mHERE 2\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     37\u001b[0m \u001b[39m# Pass through fusion layers\u001b[39;00m\n","\u001b[0;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 3 and 2"]}],"source":["prev_dev_acc = 0\n","dev_acc = 0\n","# try:\n","for epoch in range(EPOCHS):\n","    # Train model\n","    fusion.train()\n","    train_acc, train_loss = train_model(fusion)\n","    print(f\"Epoch {epoch+1}/{EPOCHS}, Train Loss = {train_loss:.4f}, Train Accuracy = {train_acc:.4f}\")\n","\n","    # Evaluate model\n","    fusion.eval()\n","    dev_acc, dev_loss = eval_model(fusion)\n","    print(f\"Epoch {epoch+1}/{EPOCHS}, Dev Loss = {dev_loss:.4f}, Dev Accuracy = {dev_acc:.4f}\")\n","\n","    # Save best model\n","    prev_dev_acc = save_model(prev_dev_acc, dev_acc, epoch + 1, fusion, optimizer)\n","        \n","# except Exception as e:\n","#     # Log the exception\n","#     print(e)\n","\n","#     # Save best model\n","#     prev_dev_acc = save_model(prev_dev_acc, dev_acc, epoch, fusion, optimizer)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"hmc","language":"python","name":"venv"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":4}
