{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nfrom torchvision import models\nfrom transformers import BertTokenizer, VisualBertModel, logging\nfrom PIL import Image\nfrom tqdm import tqdm\nimport os\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nlogging.set_verbosity_error()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_json(\"/kaggle/input/facebook-hmcwa/facebook/train.json\")\ndev_df = pd.read_json(\"/kaggle/input/facebook-hmcwa/facebook/dev.json\")\ntrain_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Some global variables\nBATCH_SIZE = 16\nEPOCHS = 10\nROOT_PATH = '/kaggle/input/facebook-hmcwa/facebook'\nIMAGE_SIZE = 224*224\nNUM_CLASSES = 2\nTEXTUAL_DIMENSION = 512\nVISUAL_DIMENSION = 512\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else'cpu')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize the dataset and maintain the dataloader\nclass DynamicDataset(Dataset):\n    def __init__(self, json_path, transform = None):\n        self.df = pd.read_json(json_path)\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        img_path = self.df.loc[index, 'img']\n        img_file = os.path.join(ROOT_PATH, img_path)\n        image = Image.open(img_file).convert(\"RGB\")\n        if self.transform is not None:\n            image = self.transform(image)\n        \n        text = self.df.loc[index, 'text']\n        if 'label' not in self.df.columns:\n            return image, text\n        label = self.df.loc[index, 'label']\n\n        return image ,text, label","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define a transform function for image preprocessing\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\n# Create objects of each set of data\ntrain_data = DynamicDataset(os.path.join(ROOT_PATH, 'train.json'), transform = transform)\ndev_data = DynamicDataset(os.path.join(ROOT_PATH, 'dev.json'), transform = transform)\n\n# Create a dataloader\ntrain_loader = DataLoader(train_data, batch_size = BATCH_SIZE, shuffle = True)\ndev_loader = DataLoader(dev_data, batch_size = BATCH_SIZE, shuffle = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Visual_Feature(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        # Define resnet50 model\n        resnet50 = models.resnet50(weights = models.ResNet50_Weights.DEFAULT)\n        convolution_layers = nn.Sequential(\n            nn.Conv2d(2048, 1024, kernel_size=(3, 3), stride = (1, 1), padding = (1, 1)),\n            nn.ReLU(),\n            nn.Conv2d(1024, 512, kernel_size=(3, 3), stride = (1, 1), padding = (1, 1)),\n            nn.ReLU(),\n        )\n\n        self.resnet50 = nn.Sequential(*list(resnet50.children())[:-1])\n        self.convolution_layers = convolution_layers\n\n    def get_visual_features(self, images):\n        # Extract visual features from resnet50 model\n        visual_features = self.convolution_layers(self.resnet50(images))\n        visual_features = visual_features.view(visual_features.size(0), -1)\n\n        return visual_features","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Textual_Feature(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        # Define virtual bert model\n        virtual_bert = VisualBertModel.from_pretrained('uclanlp/visualbert-vqa')\n        dense_layers = nn.Sequential(\n            nn.Linear(768, 512),\n            nn.ReLU(),\n        )\n\n        self.virtual_bert = virtual_bert\n        self.dense_layers = dense_layers\n\n        # Define tokenizer\n        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n    \n    def get_textual_features(self, texts):\n        # Define indices and attention mask\n        inputs = self.tokenizer.batch_encode_plus(texts, padding = True, return_tensors = 'pt')\n        input_ids = inputs['input_ids'].to(DEVICE)\n        attention_mask = inputs['attention_mask'].to(DEVICE)\n\n        # Extract textual features from virtual bert model\n        textual_features = self.virtual_bert(input_ids = input_ids, attention_mask = attention_mask, return_dict = False)\n        textual_features = textual_features[0][:, 0, :] # Extract the first token of last hidden state\n        textual_features = self.dense_layers(textual_features)\n\n        return textual_features","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test visual bert (WORKS)\nvbert = Textual_Feature()\nvbert.to(DEVICE)\ntext = [\"This is a test sentence\", \"This is another test sentence\"]\ntextual_features = vbert.get_textual_features(text)\nprint(textual_features.shape) # Shape of last hidden state","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test resnet50 (WORKS)\nresnet50 = Visual_Feature()\nresnet50.to(DEVICE)\nimage = Image.open('/kaggle/input/facebook-hmcwa/facebook/dev/hateful/01456.png')\nimage = transform(image).reshape(1, 3, 224, 224)\nvisual_features = resnet50.get_visual_features(image.to(DEVICE))\nprint(visual_features.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Fusion(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        # Define fusion layers\n        fusion_layers = nn.Sequential(\n            nn.Linear((VISUAL_DIMENSION + TEXTUAL_DIMENSION), 512),\n            nn.ReLU(),\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Linear(64, 32),\n            nn.ReLU(),\n            nn.Linear(32, 1),\n        )\n\n        self.fusion_layers = fusion_layers\n    \n    def forward(self, images, texts):\n        # Initialize text and visual classes\n        visual_class = Visual_Feature().to(DEVICE)\n        textual_class = Textual_Feature().to(DEVICE)\n\n        # Extract visual and textual features\n        visual_features = visual_class.get_visual_features(images)\n        textual_features = textual_class.get_textual_features(texts)\n\n        # Concatenate visual and textual features\n        features = torch.cat((visual_features, textual_features), dim = 1)\n\n        # Pass through fusion layers\n        output = self.fusion_layers(features)\n\n        return output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define fusion model\nfusion = Fusion()\nfusion.to(DEVICE)\n\n# Define loss function and optimizer\ncriterion = nn.BCEWithLogitsLoss()\n\n# Define optimizer\noptimizer = optim.Adam(fusion.parameters(), lr = 0.01)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fusion.train()\ntrain_loss = 0\ntrain_acc = 0\nfor images, texts, labels in tqdm(train_loader):\n    images = images.to(DEVICE)\n    labels = torch.reshape(labels, (-1, 1)).to(dtype = torch.float32, device = DEVICE)\n\n    optimizer.zero_grad()\n    outputs = fusion(images, texts)\n    loss = criterion(outputs, labels)\n    loss.backward()\n    optimizer.step()\n    train_loss += loss.item() * images.size(0)\n    train_acc += torch.sum(torch.max(outputs, dim = 1)[1] == labels)\n    \ntrain_loss = train_loss / len(train_data)\ntrain_acc = train_acc / len(train_data)\nprint(f\"Train Loss = {train_loss:.4f}, Train Accuracy = {train_acc:.4f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}